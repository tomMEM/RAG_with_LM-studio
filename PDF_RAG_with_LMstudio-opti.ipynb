{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa2b9045-0485-4252-8348-680b126b591a",
   "metadata": {},
   "source": [
    "Scientific Publications as PDFs, Text Extraction, Storage SQL database, Chunks, Embedding and Vectorization, Retrieval, GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f4f4c-7300-4cb9-8a96-31b4fec74b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not required, but can be usefull if a Cell need local proxy connection to Docker or LM Studio\n",
    "'''\n",
    "import os\n",
    "os.environ['http_proxy']=\"http://localhost:1238\"\n",
    "os.environ['https_proxy']=\"http://localhost:1238\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f32765-9ce2-4a03-b023-c99450f64562",
   "metadata": {},
   "source": [
    "## 1<sup>st</sup> Setting Up PDF Collection and DATABASE Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1370d05a-0efb-4826-998d-f9db3d87b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "%run assets/func_inputoutput.py\n",
    "# Setup paths\n",
    "cwd = os.getcwd()\n",
    "print(\"Current working directory:\", cwd)\n",
    "# Define pdf file directory\n",
    "pdf_collection_path = 'eGRASP'\n",
    "pdf_path = os.path.join(cwd, \"docs\", pdf_collection_path)\n",
    "# Define database directory\n",
    "base_name = 'eGRASP.db'#'LigaseE3_7.db'\n",
    "database_name = os.path.join(cwd, \"docs\", pdf_collection_path, base_name)\n",
    "# Create settings dictionary\n",
    "settings = {\n",
    "    'working_directory': cwd,\n",
    "    'pdf_collection_path': pdf_collection_path,\n",
    "    'pdf_path': pdf_path,\n",
    "    'base_name': base_name,\n",
    "    'database_name': database_name,\n",
    "    'additional': 'value1'  # \n",
    "}\n",
    "# Save settings to a file\n",
    "save_settings(settings)\n",
    "# Later on, load settings and unpack directly into variables\n",
    "working_directory, pdf_collection_path, pdf_path, base_name, database_name, remaining_settings = load_settings()\n",
    "# Print the loaded settings\n",
    "print(f'\\nworking_directory {working_directory}')\n",
    "print(f\"\\nPDF Collection Path: {pdf_collection_path}\")\n",
    "print(f\"\\npdf_path: {pdf_path}\")  \n",
    "print(f\"\\nbase_name: {base_name}\")  \n",
    "print(f\"\\ndatabase_name: {database_name}\")  \n",
    "print(f\"\\nRemaining Settings: {remaining_settings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626c076e-c7c8-472f-a57e-f8f27e1e0d03",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1f44a23-c9cd-43c5-be3d-24fe4ed99cc5",
   "metadata": {},
   "source": [
    "## Read PDF and Extract TEXT, Stores in a SQLite3 database (db): GROBID based approach\n",
    " * If use text file with exported References and their Abstracts from Endnote then GROBID is not required\n",
    " * Endnote export file for now just hardwired in line 295"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c70f1c-43c1-4c48-a20a-fdfa32042b39",
   "metadata": {},
   "source": [
    "# Instructions for Running GROBID Docker\n",
    "1. Download the small GROBID docker image and copy `config.json` into the Jupyter notebook working directory.\n",
    "2. Run local Docker:\n",
    "    - Start Docker for Windows.\n",
    "    - Use CMD.\n",
    "3. Execute the following command to run the docker container:\n",
    "    ```bash\n",
    "    docker run --rm --init --ulimit core=0 -p 8070:8070 lfoppiano/grobid:0.8.0\n",
    "   OR\n",
    "    docker run --rm --init --ulimit core=0 -p 8070:8070 lfoppiano/grobid:latest-crf\n",
    "    ```\n",
    "4. Check the running docker with [http://localhost:8070](http://localhost:8070) and ensure this matches the one in your `config.json`.\n",
    "5. path to the PDF collections and database are setted above   \n",
    "6. Choose to recreate the SQL (y/n) or just reload the current one.\n",
    "7. Note that processing 70 PDFs can take over 10 minutes.\n",
    "8. Wait for the console message: \"Finished\".\n",
    "9. If you encounter an XML error or some \"localhost\" mix up, ensure you stay connected with the docker by avoiding proxy configurations elsewhere.\n",
    "\n",
    "### Importing Literature Lists from EndNote (with Abstracts)\n",
    "\n",
    "Here's how to import your EndNote library, including abstracts, into this application:\n",
    "\n",
    "1. **Insert References into Word:** In EndNote, select the references you want to import and insert them into a Microsoft Word document.\n",
    "2. **Create a \"Show All Fields\" Bibliography:**  In Word, generate a bibliography using the \"Show All Fields\" style. This will include the abstracts and clearly labeled field names (e.g., \"Title:\", \"Abstract:\", etc.).\n",
    "3. **Save as UTF-8 Text File:** Save the Word document as a plain text file (`.txt`) with UTF-8 encoding.  This ensures proper character encoding.\n",
    "4. **Link the Script:**  <span style=\"color:red;\">Specify the path to the saved text file in your script (see line 272).</span>\n",
    "5. **Create from Text File:** Run the script.  Select the option to create a database from the text file.  This method bypasses GROBID; however, GROBID initialization might still run in the background.### Importing Literature Lists from EndNote (with Abstracts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1b0aba-1387-478f-8d69-8e7e9465a4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66276dc1-86f8-49fd-9d6a-fae3f53e5c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from grobid_client.grobid_client import GrobidClient\n",
    "import json\n",
    "import grobid_tei_xml\n",
    "import urllib3\n",
    "import requests\n",
    "import re\n",
    "\n",
    "%run assets/func_inputoutput.py\n",
    "# Disable warnings and proxy settings\n",
    "os.environ['NO_PROXY'] = 'localhost,127.0.0.1'\n",
    "urllib3.disable_warnings()\n",
    "urllib3.util.connection.is_connection_dropped = lambda conn: False\n",
    "\n",
    "# Configure requests to ignore proxies\n",
    "requests.Session().trust_env = False\n",
    "\n",
    "\n",
    "working_directory, pdf_collection_path, pdf_path, base_name, database_name, remaining_settings = load_settings()\n",
    "\n",
    "class GrobidAuthor:\n",
    "    def __init__(self, full_name):\n",
    "        self.full_name = full_name\n",
    "\n",
    "class GrobidBiblio:\n",
    "    def __init__(self, index, authors, title, date, volume, pages, issue, journal, doi):\n",
    "        self.index = index\n",
    "        self.authors = authors\n",
    "        self.title = title\n",
    "        self.date = date\n",
    "        self.volume = volume\n",
    "        self.pages = pages\n",
    "        self.issue = issue\n",
    "        self.journal = journal\n",
    "        self.doi = doi\n",
    "\n",
    "def extract_bibliographic_details(biblios, print_choice=False):\n",
    "    for biblio in biblios:\n",
    "        if print_choice:\n",
    "            print(\"Index\", biblio.index, \"| Title:\", biblio.title)\n",
    "            print(\"Authors:\")\n",
    "            for author in biblio.authors:\n",
    "                print(\"-\", author.full_name)\n",
    "\n",
    "            print(\"Date:\", biblio.date)\n",
    "            print(\"Volume:\", biblio.volume)\n",
    "            print(\"Pages:\", biblio.pages)\n",
    "            print(\"Journal:\", biblio.journal)\n",
    "            print(\"Doi:\", biblio.doi)\n",
    "            print()\n",
    "    \n",
    "    ref_list = '*'.join([\n",
    "        f\" * Index: {biblio.index} | Title: {biblio.title} | Authors: {', '.join([author.full_name for author in biblio.authors])} | Date: {biblio.date} | Volume: {biblio.volume} | Pages: {biblio.pages} | Journal: {biblio.journal} | Doi: {biblio.doi}\\|\"\n",
    "        for biblio in biblios\n",
    "    ])\n",
    "    return ref_list\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "def parse_structured_data(text_path):\n",
    "    records = []\n",
    "    \n",
    "    with open(text_path, 'r', encoding='utf-8') as file:\n",
    "        \n",
    "        text = file.read()\n",
    "        records_text = text.split(\"\\n\\n\")\n",
    "        for record_text in records_text:\n",
    "            record = {}\n",
    "\n",
    "            match = re.search(r\"Record Number:\\s*(\\d+)\", record_text)\n",
    "            if match:\n",
    "                record['Record Number'] = match.group(1)\n",
    "\n",
    "            match = re.search(r\"Title:\\s*(.*)\", record_text)\n",
    "            if match:\n",
    "                record['Title'] = match.group(1).strip()\n",
    "\n",
    "            match = re.search(r\"Author:\\s*(.*)\", record_text)\n",
    "            if match:\n",
    "                record['Author'] = match.group(1).strip()\n",
    "\n",
    "            match = re.search(r\"DOI:\\s*(.*)\", record_text)\n",
    "            if match:\n",
    "                record['DOI'] = match.group(1).strip()\n",
    "            \n",
    "            match = re.search(r\"Abstract:\\s*(.*)\", record_text)\n",
    "            if match:\n",
    "                record['Abstract'] = match.group(1).strip()\n",
    "                \n",
    "            match = re.search(r\"Year:\\s*(.*)\", record_text)\n",
    "            if match:\n",
    "                record['Date'] = match.group(1).strip()\n",
    "\n",
    "            match = re.search(r\"Journal:\\s*(.*)\", record_text)\n",
    "            if match:\n",
    "                record['Journal'] = match.group(1).strip()\n",
    "        \n",
    "            # Set empty values for fields not present in the text file\n",
    "            record['Citations'] = \"\"\n",
    "            \n",
    "            record['Body'] = \"\"\n",
    "            record['Refs'] = \"\"\n",
    "\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "def handle_database_existence(database_name):\n",
    "    if os.path.exists(database_name):\n",
    "        while True:\n",
    "            choice = input(\"\\nThe database file already exists. What do you want to do? \\n\"\n",
    "                           \"1. Create a new database\\n\"\n",
    "                           \"2. Append to the existing database\\n\"\n",
    "                           \"3. Do nothing (use the existing database)\\n\"\n",
    "                           \"Enter your choice (1-3): \")\n",
    "            if choice in ['1', '2', '3']:\n",
    "                break\n",
    "            print(\"Invalid choice! Please enter 1, 2, or 3.\")\n",
    "        return choice\n",
    "    else:\n",
    "        return '1'  # Create a new database if it doesn't exist\n",
    "\n",
    "def main():\n",
    "    # Initialize Grobid client\n",
    "    #client = GrobidClient(config_path=\"config.json\")\n",
    "     # Check if the database file exists\n",
    "        \n",
    "    print(f'database_name {database_name}')\n",
    "    # Overwrite or append choice (only if the database exists)\n",
    "    if os.path.exists(database_name):\n",
    "        while True:\n",
    "            choice = input(\"\\nDatabase already exists. Do you want to:\\n\"\n",
    "                           \"1. Overwrite the existing database\\n\"\n",
    "                           \"2. Append to the existing database\\n\"\n",
    "                           \"Enter your choice (1/2): \")\n",
    "            if choice in ['1', '2']:\n",
    "                break\n",
    "            print(\"Invalid choice! Please enter 1 or 2.\")\n",
    "\n",
    "        if choice == '1':\n",
    "            # Overwrite:\n",
    "            # Initialize Grobid client\n",
    "            #client = GrobidClient(config_path=\"config.json\")\n",
    "            #conn.close()\n",
    "            conn = sqlite3.connect(database_name)\n",
    "            #cursor = conn.cursor()\n",
    "            conn.close()\n",
    "            \n",
    "            os.remove(database_name)\n",
    "            print(\"Overwriting existing database...\")\n",
    "            id_key=1           #conn = sqlite3.connect(database_name)\n",
    "            #cursor = conn.cursor()\n",
    "            #cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "            #result = cursor.fetchone()\n",
    "        if choice == '2':\n",
    "            # Append: Connection already established\n",
    "            # Determine starting ID\n",
    "                # Initialize Grobid client\n",
    "            #client = GrobidClient(config_path=\"config.json\")\n",
    "            conn = sqlite3.connect(database_name)\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Check if the table exists\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "            result = cursor.fetchone()\n",
    "            table_name = result[0]\n",
    "            cursor.execute(f\"SELECT MAX(ID) FROM {table_name}\")\n",
    "            max_id = cursor.fetchone()[0]\n",
    "            id_key = max_id + 1 if max_id is not None else 1\n",
    "    else:\n",
    "            id_key=1 \n",
    "            print(\"Create Database \")\n",
    "            \n",
    "    conn = sqlite3.connect(database_name)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Check if the table exists\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    result = cursor.fetchone()\n",
    "    \n",
    "    \n",
    "    if result is None:\n",
    "        table_name = \"document_table\"\n",
    "        cursor.execute(f'''CREATE TABLE {table_name}\n",
    "                        (ID INTEGER PRIMARY KEY,\n",
    "                        Title TEXT,\n",
    "                        Authors TEXT,\n",
    "                        DOI TEXT,\n",
    "                        Citations INTEGER,\n",
    "                        Abstract TEXT,\n",
    "                        Body TEXT,\n",
    "                        Date TEXT,\n",
    "                        Record_Number INTEGER,\n",
    "                        Refs TEXT,\n",
    "                        Journal TEXT)''')\n",
    "        print(\"Created new table:\", table_name)\n",
    "    else:\n",
    "        table_name = result[0]\n",
    "        print(\"datbase exists:\", table_name)\n",
    "    \n",
    "    #conn.close()\n",
    "        \n",
    "    # Add processing mode selection\n",
    "    while True:\n",
    "        processing_mode = input(\"\\nSelect processing mode:\\n1. Grobid extraction\\n2. Plain text PDF extraction\\n3. Both\\nEnter choice (1/2/3): \")\n",
    "        if processing_mode in ['1', '2', '3']:\n",
    "            break\n",
    "        print(\"Invalid choice! Please enter 1, 2, or 3.\")\n",
    "\n",
    "    # Process PDFs based on selected mode\n",
    "    if processing_mode in ['1', '3']:\n",
    "        # Existing Grobid processing logic\n",
    "        client = GrobidClient(config_path=\"config.json\")\n",
    "        if not os.path.exists(pdf_path):\n",
    "            print(f\"Error: Directory {pdf_path} does not exist!\")\n",
    "            conn.close()\n",
    "            return\n",
    "\n",
    "        # Loop through PDF files\n",
    "        for filename in os.listdir(pdf_path):\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                try:\n",
    "                    print(f\"Processing filename: {filename}\")\n",
    "                    file_path = os.path.join(pdf_path, filename)\n",
    "\n",
    "                    # Process PDF\n",
    "                    pdf_file, status, text = client.process_pdf(\n",
    "                        \"processFulltextDocument\", \n",
    "                        file_path, \n",
    "                        generateIDs=True, \n",
    "                        consolidate_header=True, \n",
    "                        consolidate_citations=True, \n",
    "                        include_raw_citations=True, \n",
    "                        include_raw_affiliations=True, \n",
    "                        tei_coordinates=True,                          \n",
    "                        segment_sentences=True\n",
    "                    )\n",
    "\n",
    "                    # Verify text extraction\n",
    "                    if not text or text.strip() == '':\n",
    "                        raise ValueError(\"No text extracted from PDF\")\n",
    "\n",
    "                    # Parse citations\n",
    "                    try:\n",
    "                        grobid_biblios = grobid_tei_xml.parse_citation_list_xml(text)\n",
    "                        ref_list = extract_bibliographic_details(grobid_biblios, print_choice=False)\n",
    "                    except Exception as cite_error:\n",
    "                        print(f\"Citation parsing error: {cite_error}\")\n",
    "                        ref_list = \"No references parsed\"\n",
    "                    \n",
    "                    \n",
    "                    # Parse document\n",
    "                    doc = grobid_tei_xml.parse_document_xml(text)\n",
    "                    title = doc.header.title if hasattr(doc.header, 'title') else \"No Title\"\n",
    "                    authors = ';'.join([a.full_name for a in doc.header.authors]) if hasattr(doc.header, 'authors') else \"No Authors\"\n",
    "                    doi = str(doc.header.doi) if hasattr(doc.header, 'doi') else \"No DOI\"\n",
    "                    citations = str(len(doc.citations)) if hasattr(doc, 'citations') else \"0\"\n",
    "                    abstract = doc.abstract if hasattr(doc, 'abstract') else \"No Abstract\"\n",
    "                    body = doc.body if hasattr(doc, 'body') else \"No Body\"\n",
    "                    date = doc.header.date if hasattr(doc.header, 'date') else \"No Date\"\n",
    "                    journal =doc.header.journal if hasattr(doc.header, 'date') else \"No Date\"\n",
    "                    #----------\n",
    "                    # --- Date Extraction (Improved) ---\n",
    "                    #date_element = doc.header.source_desc.analytic.publication_stmt.date\n",
    "                    #date = parse_grobid_date(date_element) # Use the parsing function\n",
    "\n",
    "                    print(f'date {date}\\n ')\n",
    "                    \n",
    "                    #date=doc.date\n",
    "                    #print(f'date {date}')\n",
    "\n",
    "                    # Insert into database\n",
    "                    cursor.execute(f'''\n",
    "                        INSERT INTO {table_name} (ID, Title, Authors, DOI, Citations, Abstract, Body, Date, Refs, Journal)\n",
    "                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                    ''', (id_key, title, authors, doi, citations, abstract, body, date, ref_list, journal))\n",
    "\n",
    "                    # Commit and increment\n",
    "                    conn.commit()\n",
    "                    id_key += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    # Final record count\n",
    "    #cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "    #count = cursor.fetchone()[0]\n",
    "   # print(\"Total records:\", count)\n",
    "\n",
    "    # Close connection\n",
    "    #conn.close()\n",
    "    #print(\"Finished processing\")\n",
    "\n",
    "    if processing_mode in ['2', '3']:\n",
    "        ## Process the structured data file\n",
    "        print(f'table_name {table_name}')\n",
    "        #text_path = input(\"Enter the path to the directory containing text files: \")\n",
    "        text_path=r'\\users\\na\\Python\\envs\\GPT\\openai-quickstart-node-master\\PDF_screen\\Langchain\\LangChain-Chat-with-Your-Data-main\\docs\\eGRASP\\egrasp.txt'\n",
    "        print(f'Note: hardwired path and file name in line 295 \\n')\n",
    "        records = parse_structured_data(text_path)\n",
    "        \n",
    "        # Insert records into the database\n",
    "        \n",
    "        for record in records:\n",
    "            \n",
    "            cursor.execute(f'''\n",
    "                INSERT INTO document_table (ID, Title, Authors, DOI, Citations, Abstract, Date, Record_Number, Journal)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                id_key,#record.get('Record Number', None),\n",
    "                record.get('Title', None),\n",
    "                record.get('Author', None),\n",
    "                record.get('DOI', None),\n",
    "                record.get('Citations', None),\n",
    "                record.get('Abstract', None),\n",
    "                record.get('Date', None),\n",
    "                record.get('Record Number', None),\n",
    "                record.get('Journal', None)\n",
    "            ))\n",
    "                \n",
    "            # Commit and increment\n",
    "            conn.commit()\n",
    "            \n",
    "            print(f'id {id_key}')\n",
    "            id_key += 1\n",
    "            \n",
    "            \n",
    "\n",
    "    # Final record count\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(\"Total records:\", count)\n",
    "\n",
    "    # Close connection\n",
    "    conn.close()\n",
    "    print(\"Finished processing\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #conn.close()\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b3b4f3-ca02-4fcd-b653-133fd323d9be",
   "metadata": {},
   "source": [
    "# Check records of SQL database: \n",
    "### Can also be used alone without Docker, once SQL database was created\n",
    " * if start from here, then run settings in second cell at start \n",
    " * database\n",
    "    * base_name='sample.db'\n",
    "    * database_name=os.path.join(cwd, \"docs\", pdf_collection_path, base_name) #database\n",
    " * Enter the record number or 'q' to quit: After: Enter 'n' for next choice or 'q' for quitEnter the record number or 'q' to quit: After: Enter 'n' for next choice or 'q' for quit# Check of SQL database: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c2768e-cfd6-49c2-a97d-35137526a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#database structure\n",
    "skip=True\n",
    "if not skip:\n",
    "    import importlib\n",
    "    from assets import func_database  # Import the entire module from assets func_database.py\n",
    "    importlib.reload(func_database) # in case of editing the functions\n",
    "    # Call the function initially\n",
    "    func_database.database_structure(database_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbe3eac-5e5a-4a2a-82c3-bb14f56d83c1",
   "metadata": {},
   "source": [
    "# Dashboard to see contents of database\n",
    " * requires that database_name was set before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8216d6-8e6f-429f-83d5-3b1c06917d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "def on_record_selected(change):\n",
    "    selected_option = change['new']\n",
    "    \n",
    "    if selected_option in record_options:\n",
    "        record_id, record_number = record_options[selected_option]\n",
    "        record_number_display.value = f\"Record Number: {record_number}\"\n",
    "    else:\n",
    "        record_number_display.value = \"\"\n",
    "\n",
    "    return\n",
    "\n",
    "#js_close_tab = \"\"\"<script>window.close();</script>\"\"\"\n",
    "\n",
    "def close_app(event):\n",
    "    try:\n",
    "        conn.close()\n",
    "        message_area.value = \"Database closed successfully. You may close this tab/window.\"\n",
    "    except Exception as e:\n",
    "        message_area.value = f\"Error closing database: {e}\" # Show errors to user\n",
    "\n",
    "pn.extension()\n",
    "# --- Database Connection ---\n",
    "try:\n",
    "    with sqlite3.connect(database_name) as conn: # Replace with your database name\n",
    "        cursor = conn.cursor()\n",
    "        conn = sqlite3.connect(database_name)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = cursor.fetchall()\n",
    "        if not tables:\n",
    "            raise ValueError(\"No tables found in the database.\")\n",
    "        table_name = tables[0][0]\n",
    "\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        row_count = cursor.fetchone()[0]\n",
    "        \n",
    "        if row_count == 0:\n",
    "            raise ValueError(f\"Table '{table_name}' is empty.\")\n",
    "\n",
    "        \n",
    "        user_input = input(\"dashboard: 'n' : 'y'\")\n",
    "        if user_input.lower() == 'y':\n",
    "\n",
    "            record_ids_select = pn.widgets.Select(\n",
    "                name='Record ID',\n",
    "                sizing_mode='stretch_width',\n",
    "                styles={\n",
    "                    'background-color': 'white',\n",
    "                    'color': 'black',\n",
    "                    'border-color': '#555'\n",
    "                }\n",
    "            )\n",
    "            record_number_display = pn.widgets.StaticText(\n",
    "                name='Record Number',\n",
    "                value=\"\",\n",
    "                sizing_mode='stretch_width',\n",
    "                styles={\n",
    "                    'background-color': 'white',\n",
    "                    'color': 'black',\n",
    "                    'border-color': '#555',\n",
    "                    'padding': '5px'\n",
    "                }\n",
    "            )\n",
    "            output_area = pn.widgets.TextAreaInput(\n",
    "                value=\"\",\n",
    "                sizing_mode='stretch_width',\n",
    "                height=400,\n",
    "                styles={\n",
    "                    'background-color': 'white',\n",
    "                    'color': 'black',\n",
    "                    'font-family': 'monospace',\n",
    "                    'font-size': '14px',\n",
    "                    'border-color': '#555',\n",
    "                    'padding': '10px'\n",
    "                },\n",
    "                disabled=True\n",
    "            )\n",
    "            def populate_record_ids():\n",
    "                try:\n",
    "                    cursor.execute(f\"SELECT ID, Title FROM {table_name}\")\n",
    "                    records = cursor.fetchall()\n",
    "                    if not records:\n",
    "                        output_area.value = \"No records found.\"\n",
    "                        record_ids_select.options = []\n",
    "                        return {}\n",
    "                    record_options = {}\n",
    "                    for row in records:\n",
    "                        record_id = row[0]\n",
    "                        title = row[1] if row[1] is not None else \"No abstract available\"\n",
    "                        record_options[f\"ID {record_id}: {title}\"] = (record_id)\n",
    "                    return record_options\n",
    "                except Exception as e:\n",
    "                    print(f\"Error populating record IDs: {e}\")\n",
    "                    output_area.value = f\"Error: {e}\"\n",
    "                    return {}\n",
    "            record_options = populate_record_ids()\n",
    "            if record_options:\n",
    "                record_ids_select.options = record_options\n",
    "                record_ids_select.value = list(record_options.values())[0]  # Set initial value\n",
    "            else:\n",
    "                record_ids_select.options = []\n",
    "                output_area.value = \"No records found or error in fetching records.\"\n",
    "            @pn.depends(record_ids_select.param.value, watch=True)\n",
    "            def update_output(record_id):\n",
    "                if record_id is None:\n",
    "                    output_area.value = \"No record selected or table is empty.\"\n",
    "                    record_number_display.value = \"\"\n",
    "                    return\n",
    "                try:\n",
    "                    cursor.execute(f\"SELECT Abstract, Authors, Body, Date, Record_Number, Journal FROM {table_name} WHERE ID = ?\", (record_id,))\n",
    "                    record_data = cursor.fetchone()\n",
    "                    if record_data:\n",
    "                        abstract_text = str(record_data[0])\n",
    "                        body_text = str(record_data[2])\n",
    "                        date_text = str(record_data[3])\n",
    "                        recordnr_integer = str(record_data[4])\n",
    "                        journal_text=str(record_data[5])\n",
    "                        output_area.value = f'Journal: {journal_text}\\nAbstract: {abstract_text}\\n\\nBody: {body_text}'\n",
    "                        record_number_display.value = f\"Record Number: {recordnr_integer} | Author: {record_data[1]} | Date: {date_text}\"\n",
    "                    else:\n",
    "                        output_area.value = f\"Record with ID {record_id} not found.\"\n",
    "                        record_number_display.value = \"\"\n",
    "                except Exception as e:\n",
    "                    output_area.value = f\"Error retrieving record: {e}\"\n",
    "                    record_number_display.value = \"\"\n",
    "            if record_options:\n",
    "                update_output(record_ids_select.value)\n",
    "\n",
    "            # Create a close button\n",
    "            #close_button = pn.widgets.Button(label='Close', button_type='danger')\n",
    "            close_button = pn.widgets.Button(name='Close', button_type='danger')  # Set initial label\n",
    "            close_button.on_click(close_app)\n",
    "            message_area = pn.widgets.StaticText(value=\"\")  # Or pn.pane.Markdown(\"\")\n",
    "            \n",
    "\n",
    "            dashboard_layout = pn.Column(\n",
    "                pn.pane.Markdown(\n",
    "                    \"## Record Viewer\",\n",
    "                    styles={'color': 'black', 'background-color': 'white'}\n",
    "                ),\n",
    "                record_ids_select,\n",
    "                record_number_display,\n",
    "                output_area,\n",
    "                close_button,\n",
    "                message_area,\n",
    "                sizing_mode='stretch_width'\n",
    "            )\n",
    "            dashboard = pn.panel(dashboard_layout)\n",
    "            dashboard.show()\n",
    "        else:\n",
    "            print(\"Dashboard creation cancelled.\")\n",
    "        \n",
    "except Exception as e: # Catches errors during database operations\n",
    "    print(f\"Database Error: {e}\")\n",
    "# Ensure connection is closed if not already closed\n",
    "# conn.close()  # Uncomment this line if you want to ensure it closes when the script ends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f3733f-f5e6-4135-bb83-60dc79a9869f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42c285f4-fed9-45fd-8fa0-2085191f07e4",
   "metadata": {},
   "source": [
    "# This document explains how to run LM Studio and configure your vector database, whether you're using a pre-existing SQLite database or creating a new one.\n",
    "\n",
    "## Using an Existing SQLite Database\n",
    "\n",
    "If you have an existing SQLite database, GROBID is not required for this step. You can close or quit any applications related to the SQLite database creation process to free up system resources (RAM).\n",
    "\n",
    "## Creating a New Vector Database\n",
    "\n",
    "If you don't have a pre-existing SQLite database, follow these steps to create a new vector database using Chroma (version 0.5.0) and LangChain (version 0.2.3):This document explains how to run LM Studio and configure your vector database, whether you're using a pre-existing SQLite database or creating a new one.\n",
    "\n",
    "## Using an Existing SQLite Database\n",
    "\n",
    "If you have an existing SQLite database, GROBID is not required for this step. You can close or quit any applications related to the SQLite database creation process to free up system resources (RAM).\n",
    "\n",
    "## Creating a New Vector Database\n",
    "\n",
    "If you don't have a pre-existing SQLite database, follow these steps to create a new vector database using Chroma (version 0.5.0) and LangChain (version 0.2.3):\n",
    "based on: Chroma version: 0.5.0 | Langchain version: 0.2.3\n",
    "\n",
    "1. Start LM Studio 0.2.27.\n",
    "2. Activate the Local Server Tab (\" <-> \").\n",
    "3. Select a model, e.g., \"Meta-Llama-3-8B-Instruct-Q8_0.gguf\".\n",
    "    - Choose the preset \"Llama3\".\n",
    "4. Choose a server port that does not conflict with the docker port.\n",
    "5. Load the \"Embedding Model\", e.g., \"nomic embed text v1 5 Q8_0.gguf\".\n",
    "    - Models were previously downloaded manually from Huggingface/Models/.\n",
    "6. Press \"Start Server\".\n",
    "7. Define if required the following paths in line 80-81:\n",
    "    ```python\n",
    "    \n",
    "    user_path = os.path.join(os.getcwd(), \"docs\", pdf_path, \"sample\")\n",
    "    temp_dir = os.path.join(os.getcwd(), \"docs\", \"temp\")\n",
    "    \n",
    "    ```\n",
    "8. Run the cell:\n",
    "    - If there is an error indicating the base is in use, ensure the path is correct and start from the cell \"Check SQL Database\" after restarting the kernel.\n",
    "9. If there is a connection error to LM, remove any other open proxy or browser proxy settings in \"Settings\" in browser proxy settings, restart the kernel, and start from \"Check SQL Database\", or disable warnings and proxy settings in line 130 \n",
    "10. After `embed_and_chunk` with adding metadate, creating the vector database, set mode in line 186 to `load` or ``append``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a54ab1-d09d-43f3-b76e-e4b6c7e7bc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Import here\n",
    "import sqlite3\n",
    "\n",
    "%run assets/func_inputoutput.py\n",
    "\n",
    "def load_and_process_docs_from_sqlite(database_name, table_name=\"abstracts\", id_column=\"ID\", text_column=\"Abstract\"): # Customize column names\n",
    "    try:\n",
    "        conn = sqlite3.connect(database_name)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        cursor.execute(f\"SELECT {id_column}, {text_column} FROM {table_name}\")\n",
    "        results = cursor.fetchall()\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "        documents = []\n",
    "        for row in results:\n",
    "            doc_id = row[0]\n",
    "            content = row[1]\n",
    "            if content is not None and content.strip(): # Skip if text content is empty\n",
    "                metadata = {\"ID\": doc_id} # Add id to metadata, can add others as needed\n",
    "                doc = Document(page_content=content, metadata=metadata)\n",
    "                documents.append(doc)\n",
    "            else:\n",
    "                print(f\"Skipping empty or None content for ID: {doc_id}\")\n",
    "\n",
    "        return documents\n",
    "\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"SQLite error: {e}\")\n",
    "        return []  # Return empty list in case of error\n",
    "    except Exception as e:  # Catch other potential errors\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "class CustomEmbedding2:  # If you MUST keep this, initialize client here\n",
    "    def __init__(self, client):  # Pass client to the constructor\n",
    "        self.client = client  # Store the client\n",
    "        self.embeddings = []\n",
    "\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        embeddings = [self.get_embedding(text) for text in texts] # use self.get_embedding\n",
    "        self.embeddings = embeddings\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        embedding = self.get_embedding(text)  # Use self.get_embedding\n",
    "        self.embeddings = [embedding]\n",
    "        return embedding\n",
    "\n",
    "    def get_embedding(self, text, model=\"TheBloke/nomic-embed-text\"): # make this a method of the class\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        return self.client.embeddings.create(input=[text], model=model).data[0].embedding # Use self.client\n",
    "\n",
    "\n",
    "\n",
    "def embed_and_chunk2(docs, embedding, chunk_size=2000, chunk_overlap=200):\n",
    "    \"\"\"Embeds and chunks a list of Documents, returning a tuple (texts, embeddings).\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "\n",
    "    embeddings = embedding.embed_documents([doc.page_content for doc in texts])\n",
    "\n",
    "    # Store embeddings in a dictionary, keyed by document chunk ID or index\n",
    "    embedding_dict = {i: emb for i, emb in enumerate(embeddings)}  # Or use some unique ID from doc.metadata\n",
    "\n",
    "    return texts, embedding_dict\n",
    "\n",
    "def embed_and_chunk(docs, embedding, chunk_size=2000, chunk_overlap=200):\n",
    "    \"\"\"Embeds and chunks a list of Documents.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "\n",
    "    embeddings = embedding.embed_documents([doc.page_content for doc in texts])\n",
    "\n",
    "\n",
    "    embedding_dict = {i: emb for i, emb in enumerate(embeddings)}  # Still useful for efficient embedding lookup\n",
    "\n",
    "\n",
    "    # IMPORTANT: Add \"doc_id\" to each chunk's metadata:\n",
    "    for i, text in enumerate(texts):\n",
    "        text.metadata['doc_id'] = text.metadata['ID'] # Or whatever unique ID you are using from SQLite\n",
    "\n",
    "\n",
    "        text.metadata['chunk_id'] = i  # It's often helpful to have a unique chunk ID as well\n",
    "\n",
    "    return texts, embedding_dict\n",
    "\n",
    "def create_vectordb(texts, embedding, user_path=None, persist_directory=None, mode=\"create\", force_overwrite=True):\n",
    "    \"\"\"Creates, loads, or appends to a vector database.\"\"\"\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    if mode == \"create\":  # Create a new database (overwrites if exists)\n",
    "        if persist_directory is None:\n",
    "            if user_path:\n",
    "                persist_directory = os.path.join(cwd, user_path, 'chroma_db')\n",
    "            else:\n",
    "                persist_directory = os.path.join(cwd,'chroma_db')\n",
    "                raise ValueError(f\"persist_directory or user_path must be provided when creating, now used {persist_directory}\")\n",
    "                \n",
    "        if os.path.exists(persist_directory): # Crucial check for empty directory\n",
    "            if os.listdir(persist_directory): # Check if the directory is NOT empty\n",
    "                if force_overwrite:  # Only remove if force_overwrite is True and not empty\n",
    "                    shutil.rmtree(persist_directory)\n",
    "                else:\n",
    "                    raise FileExistsError(f\"Vector database directory '{persist_directory}' is not empty. Use mode='append' or set force_overwrite=True to overwrite.\")\n",
    "            else: # Directory exists but empty\n",
    "                shutil.rmtree(persist_directory) # clean since old chroma files might be in\n",
    "\n",
    "        print(\"Creating a new vector database...\")  # More informative message\n",
    "        start_time = time.time()\n",
    "        vectordb = Chroma.from_documents(texts, embedding, persist_directory=persist_directory)\n",
    "        end_time = time.time()\n",
    "        print(f\"New vector database created in {end_time - start_time:.2f} seconds. Directory: {persist_directory}\")\n",
    "\n",
    "\n",
    "\n",
    "    elif mode == \"load\":  # Load existing database\n",
    "        if persist_directory is None:\n",
    "            if user_path:\n",
    "                persist_directory = os.path.join(cwd, user_path, 'chroma_db')\n",
    "            else:\n",
    "                raise ValueError(\"persist_directory or user_path must be provided when loading\")\n",
    "\n",
    "        if not os.path.exists(persist_directory):\n",
    "            raise FileNotFoundError(f\"Vector database not found: {persist_directory}\") # Raise exception\n",
    "\n",
    "        print(f\"Loading vector database from {persist_directory}...\")\n",
    "        start_time = time.time()\n",
    "        vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "        end_time = time.time()\n",
    "        print(f\"Vector database loaded in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "    elif mode == \"append\":\n",
    "        if persist_directory is None:\n",
    "            if user_path:\n",
    "                persist_directory = os.path.join(cwd, user_path, 'chroma_db')\n",
    "            else:\n",
    "                raise ValueError(\"persist_directory or user_path must be provided when appending\")\n",
    "\n",
    "\n",
    "        if not os.path.exists(persist_directory):\n",
    "            raise FileNotFoundError(f\"Vector database not found: {persist_directory}\") # Raise exception\n",
    "\n",
    "        print(f\"Appending to vector database at {persist_directory}...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "        vectordb.add_documents(texts)\n",
    "        end_time = time.time()\n",
    "        print(f\"Documents appended in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid mode: {mode}. Choose 'create', 'load', or 'append'.\")\n",
    "\n",
    "    return vectordb, persist_directory\n",
    "\n",
    "import os\n",
    "import urllib3\n",
    "\n",
    "# Disable warnings and proxy settings\n",
    "os.environ['NO_PROXY'] = 'localhost,127.0.0.1'\n",
    "urllib3.disable_warnings()\n",
    "urllib3.util.connection.is_connection_dropped = lambda conn: False\n",
    "\n",
    "# Configure requests to ignore proxies\n",
    "import requests\n",
    "requests.Session().trust_env = False\n",
    "\n",
    "working_directory, pdf_collection_path, pdf_path, base_name, database_name,_ = load_settings()\n",
    "\n",
    "user_path = os.path.join(os.getcwd(), \"docs\", pdf_collection_path, \"sample\")\n",
    "temp_dir = os.path.join(os.getcwd(), \"docs\", \"temp\")\n",
    "\n",
    "desired_chunk_size = 2000\n",
    "chunk_overlap = 450\n",
    "\n",
    "persist_directory=user_path#temp_dir\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:1238/v1\", api_key=\"lm-studio\") # Important: Initialize client before embedding\n",
    "embedding = CustomEmbedding2(client=client)\n",
    "\n",
    "\n",
    "# Database connection and document loading:\n",
    "conn = sqlite3.connect(database_name)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "if not tables:\n",
    "    raise ValueError(\"No tables found in the database.\")\n",
    "table_name = tables[0][0]\n",
    "\n",
    "cursor.execute(f\"SELECT ID, Title, Abstract, Body FROM {table_name}\")  # Simplified query (removed Authors, DOI if not needed)\n",
    "#cursor.execute(\"SELECT ID, Title, Authors, DOI, Abstract, Body FROM {table_name}\".format(table_name=table_name))\n",
    "results = cursor.fetchall()\n",
    "\n",
    "num_original_docs = 0  # Initialize counter\n",
    "docs = []  # List to store the Document objects *before* chunking\n",
    "\n",
    "for result in results:\n",
    "    record_id, title, abstract, body = result\n",
    "    \n",
    "    #print(f\"Record_id {record_id} | title {title}\")\n",
    "    num_original_docs += 1  # Increment counter\n",
    "    if abstract is None:\n",
    "        abstract = \"\"\n",
    "    if body is None:\n",
    "        body = \"\"\n",
    "\n",
    "    combined_text = abstract + \" \" + body\n",
    "    #metadata = {'ID': record_id, 'Title': title, 'Record_Number': record_number} # Add Record_Number to metadata\n",
    "    doc = Document(page_content=combined_text, metadata={'ID': record_id, 'Title': title})\n",
    "    \n",
    "    docs.append(doc)  # Add the whole document\n",
    "print(f\"\\n Finished reading and adding Metadata to database {database_name} records \")\n",
    "\n",
    "conn.close()\n",
    "#---------------------------------------------------------------------------------\n",
    "\n",
    "#mode=\"create\"\n",
    "#mode=\"load\"\n",
    "#force_overwrite=False #when load\n",
    "\n",
    "while True:\n",
    "    mode = input(\"Enter mode ('create', 'load', or 'append'): \").lower()\n",
    "    if mode in (\"create\", \"load\", \"append\"):\n",
    "        break\n",
    "    else:\n",
    "        print(\"Invalid mode. Please enter 'create', 'load', or 'append'.\")\n",
    "\n",
    "\n",
    "# Set force_overwrite based on mode\n",
    "force_overwrite = False  # Default\n",
    "if mode == \"create\":\n",
    "    while True:\n",
    "        overwrite_choice = input(\"Overwrite existing database? (y/n): \").lower()\n",
    "        if overwrite_choice in (\"y\", \"n\"):\n",
    "            force_overwrite = (overwrite_choice == \"y\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter 'y' or 'n'.\")\n",
    "\n",
    "\n",
    "\n",
    "if not mode=='load':\n",
    "    print(f'\\ncreates texts, embedding_dict and verctordb if mode is \"load\": Current Mode: {mode}')\n",
    "    texts, embedding_dict = embed_and_chunk(docs, embedding, chunk_size=desired_chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "\n",
    "    print(f'mode {mode} | force_overwrite {force_overwrite}')\n",
    "    vectordb, _ = create_vectordb(texts, embedding, user_path=user_path, persist_directory=persist_directory, mode=mode, force_overwrite=force_overwrite)  # Pass force_overwrite\n",
    "\n",
    "else:\n",
    "    texts=[]\n",
    "    vectordb, _ = create_vectordb(texts, embedding=embedding, user_path=user_path, persist_directory=persist_directory, mode=mode, force_overwrite=force_overwrite)  # No need to provide texts when loading\n",
    "    #vectordb.persist()  # Persist the database and release locks\n",
    "\n",
    "print(f\"Number of original documents: {num_original_docs} | chunks {vectordb._collection.count()}\")  # Now you have the original count\n",
    "print(\"\\nfinished\", persist_directory, \"\\n chunks: \", len(vectordb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496ca086-497d-43e2-808e-e936785a7f84",
   "metadata": {},
   "source": [
    "# Check vectordb Structure of vectordb: Metadata etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cd5cfa-30e3-4a29-a0bf-6d4179749259",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "skip=False\n",
    "if not skip:\n",
    "    # 1. Get the correct collection *name*:\n",
    "    collection_name = next(iter(vectordb._client.list_collections())).name # Get the name as string\n",
    "\n",
    "    # 2. Get the collection *object* using the correct name:\n",
    "    collection = vectordb._client.get_collection(collection_name) # Crucial correction\n",
    "\n",
    "\n",
    "    print(f\"Collection name: {collection_name}\")\n",
    "    print(collection.count())  # Now this count will be from correct collection   \n",
    "\n",
    "    # Either get all the metadata and documents at once (less efficient for large dbs)\n",
    "    all_docs = collection.get(include=[\"documents\", \"metadatas\"])\n",
    "    if all_docs:\n",
    "        print(len(all_docs['documents']))  # Number of documents\n",
    "\n",
    "        for i in range(len(all_docs['documents'])):\n",
    "            print(f\"Metadata: {all_docs['metadatas'][i]}\")\n",
    "            #print(f\"Document {i+1}:\")\n",
    "            print(f\"Content: {all_docs['documents'][i]}\\n\\n\")\n",
    "            #print(f\"Metadata: {all_docs['metadatas'][i]}\")\n",
    "    else:\n",
    "        print(\"No documents found in the collection.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Or iterate more efficiently (for very large collections):\n",
    "    for doc in collection.get(include=[\"documents\", \"metadatas\"]):\n",
    "        print(doc)  # Now will show the actual content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b29ed3d-741c-4e8d-b311-4ed842cb4cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_vectordb(vectordb, search_field, search_value, k=3):\n",
    "    \"\"\"Searches the vector database based on user-specified field and value.\"\"\"\n",
    "\n",
    "    collection = vectordb._collection\n",
    "    results = collection.get(where={search_field: search_value}) # Get by metadata\n",
    "\n",
    "\n",
    "    if results and results['documents']:\n",
    "        print(f\"Found {len(results['documents'])} document(s) matching {search_field}: {search_value}\")\n",
    "        for i in range(len(results['documents'])):\n",
    "            print(f\"Document {i + 1}:\")\n",
    "            print(f\"Content: {results['documents'][i]}\")\n",
    "            print(f\"Metadata: {results['metadatas'][i]}\")\n",
    "\n",
    "            # Convert to LangChain Document object if needed\n",
    "            # doc = Document(page_content=results['documents'][i], metadata=results['metadatas'][i])\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "    elif search_field in ['ID', 'chunk_id']: # If not found by metadata, try similarity search\n",
    "        print(f\"No exact match found for {search_field}: {search_value}. Trying a similarity search...\")\n",
    "\n",
    "        try: # Convert to string for similarity search\n",
    "            search_query = str(search_value) # Convert search term to string\n",
    "\n",
    "            search_results = vectordb.similarity_search(search_query, k=k)\n",
    "            print(f\"Found {len(search_results)} similar document(s):\")\n",
    "            for result in search_results:\n",
    "                print(result.page_content)\n",
    "                print(result.metadata)\n",
    "                print(\"-\" * 20)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during similarity search: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"No documents found matching {search_field}: {search_value}\")\n",
    "        \n",
    "while True:\n",
    "    search_choice = input(\"Search by (1) ID or (2) chunk_id? (Enter 1 or 2, or q to quit): \")\n",
    "    if search_choice.lower() == 'q':\n",
    "        break\n",
    "\n",
    "    if search_choice in ('1', '2'):\n",
    "        search_field = \"ID\" if search_choice == '1' else \"chunk_id\"\n",
    "        try:\n",
    "            search_value = int(input(f\"Enter the {search_field} to search for: \"))\n",
    "\n",
    "            search_vectordb(vectordb, search_field, search_value)\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter an integer.\")\n",
    "    else:\n",
    "        print(\"Invalid choice. Please enter 1 or 2.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a42653-a533-4a79-84ff-e6241369dff1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Next functions for retrieval and Chat\n",
    " * -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1906c371-963d-4586-a93b-2125459ea41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "class DocumentRetriever:\n",
    "    def __init__(self, vectordb):\n",
    "        \"\"\"\n",
    "        Initializes the DocumentRetriever with a vector database.\n",
    "        \n",
    "        Parameters:\n",
    "            vectordb: The vector database used for similarity search.\n",
    "        \"\"\"\n",
    "        self.vectordb = vectordb\n",
    "    def retrieve_documents(self, query, is_first_run, k=10, method='combined'):\n",
    "        # Check for the \"do retrieval\" label\n",
    "     \n",
    "        retrieved_text = \"\"\n",
    "        refined_query = \"\"\n",
    "        \n",
    "        if '{do not use retrieval}' in query:\n",
    "            refined_query = \"Retrieval skipped as requested.\"\n",
    "            return \" \", refined_query, method\n",
    "        \n",
    "        if query.startswith(\"doc_id:\"):\n",
    "            doc_id = query.split(\":\", 1)[1].strip().split(\",\")[0]  # Extract the doc_id\n",
    "            method = \"direct_doc_id_search\"  # Define a method for clarity\n",
    "            # Call the search_vectordb method to retrieve the documents\n",
    "            similar_docs = self.search_vectordb_chat('doc_id', doc_id, k)\n",
    "            # Initialize retrieved_text\n",
    "            \n",
    "            # Format documents if any were retrieved\n",
    "            if similar_docs:\n",
    "                refined_query = \"documents found.\"\n",
    "            else:\n",
    "                refined_query = \"No documents found.\"  # Handle the case when no documents are returned\n",
    "            refined_query = f\"Searching for document with ID: {doc_id} | {refined_query}\"  # Create a simple refined query\n",
    "            # Return the retrieved text, refined query, and method\n",
    "            return similar_docs, refined_query, method\n",
    "\n",
    "        \n",
    "        if is_first_run:\n",
    "            # On first run, simply use the original query\n",
    "            refined_query = \"No retrieval\"#query\n",
    "            similar_docs = []#self.vectordb.similarity_search(refined_query, k=k)\n",
    "        else:\n",
    "            # Extract keywords or generate refined query based on the method\n",
    "            if method == 'keywords':\n",
    "                refined_query = self.extract_keywords(query)\n",
    "                \n",
    "            elif method == 'llm':\n",
    "                refined_query = self.generate_useful_query(query)\n",
    "                \n",
    "            elif method == 'combined':\n",
    "                keywords = self.extract_keywords(query)\n",
    "                refined_query = self.generate_useful_query(query)\n",
    "                refined_query += \" \" + keywords\n",
    "                \n",
    "            # Check if the refined query is meaningful\n",
    "            if self.is_query_meaningful(refined_query):\n",
    "                similar_docs = self.vectordb.similarity_search(refined_query, k=k)\n",
    "            else:\n",
    "                # Handle case where query is not meaningful\n",
    "                refined_query=\"No meaningful query could be generated. No retrieval\"\n",
    "                return \" \", refined_query, method\n",
    "        # Format documents if any were retrieved\n",
    "        if similar_docs:\n",
    "            for i, doc in enumerate(similar_docs):\n",
    "                retrieved_text += (\n",
    "                    f\"**Document {doc.metadata['doc_id']}, \"\n",
    "                    f\"Chunk {doc.metadata.get('chunk_id', 'N/A')}**:\\n{doc.page_content}\\n\\n\"\n",
    "                )\n",
    "        else:\n",
    "            return \"No documents found.\", refined_query, method\n",
    "        return retrieved_text, refined_query, method\n",
    "\n",
    "    def is_query_meaningful(self, query):\n",
    "        \"\"\"\n",
    "        Determines if the query is meaningful enough for retrieval.\n",
    "\n",
    "        Parameters:\n",
    "            query (str): The input query.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the query is meaningful, False otherwise.\n",
    "        \"\"\"\n",
    "        # Remove common non-meaningful phrases\n",
    "        if not query or query.lower() in ['no content found.', 'no keywords found']:\n",
    "            return False\n",
    "\n",
    "        # Check query length\n",
    "        if len(query.strip()) < 3:\n",
    "            return False\n",
    "\n",
    "        # Optional: Add more sophisticated meaning detection\n",
    "        # For example, check against a list of stop words or minimum information content\n",
    "        meaningless_phrases = [\n",
    "            'no content found',\n",
    "            'no keywords',\n",
    "            'not specified',\n",
    "            'empty query',\n",
    "            'no llm',\n",
    "            'no retrieval',\n",
    "            'refined query',\n",
    "            'test',\n",
    "            'tests'\n",
    "        ]\n",
    "\n",
    "        # Convert to lowercase for case-insensitive comparison\n",
    "        query_lower = query.lower()\n",
    "\n",
    "        # Check if query matches any meaningless phrases\n",
    "        if any(phrase in query_lower for phrase in meaningless_phrases):\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def extract_keywords(self, query):\n",
    "        \"\"\"\n",
    "        Extracts keywords from the query based on curly braces.\n",
    "\n",
    "        Parameters:\n",
    "            query (str): The input query.\n",
    "\n",
    "        Returns:\n",
    "            str: A string of extracted keywords or empty string.\n",
    "        \"\"\"\n",
    "        # Find keywords within curly braces\n",
    "        keywords = re.findall(r'\\{(.*?)\\}', query)\n",
    "\n",
    "        # Return joined keywords or empty string if no keywords found\n",
    "        return ', '.join(keywords) if keywords else ''\n",
    "    \n",
    "    def generate_useful_query(self, query):\n",
    "        \"\"\"\n",
    "        Generates a useful query using an LLM based on the given input.\n",
    "        \n",
    "        Parameters:\n",
    "            query (str): The input query.\n",
    "        \n",
    "        Returns:\n",
    "            str: A refined query generated by the LLM.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            instruction = \"Extract named entities and specific technical terms \\\n",
    "            from the following query that are most relevant for search. \\\n",
    "            PROVIDE ONLY the entities/terms, separated by commas. \\\n",
    "            Focus on proper nouns, specific technologies, or key concepts. \\\n",
    "            Do not use format instructions like Document Number for keywords.\"\n",
    "           \n",
    "            prompt_template = \"\"\"\n",
    "                                {instruction}\n",
    "                                Original Query: \"{query}\"\n",
    "                                Refined Query: \n",
    "                                \"\"\"\n",
    "            prompt = prompt_template.format(\n",
    "                query=query,\n",
    "                instruction=instruction\n",
    "            )\n",
    "            \n",
    "            # Call the LLM with the generated prompt\n",
    "            completion = client.chat.completions.create(\n",
    "                #model=\"LLMA/Meta-Llama-3-8B\",\n",
    "                model=\"lmstudio/Meta-Llama-3.1\",\n",
    "                #model=\"TheBloke/Mistral\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert at extracting precise semantic phrase groups from complex queries\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],              \n",
    "                temperature=0.9,\n",
    "                stream=True,\n",
    "            )\n",
    "            \n",
    "           \n",
    "            # Collect the response from the LLM\n",
    "            full_response = ''.join(\n",
    "                chunk.choices[0].delta.content \n",
    "                for chunk in completion \n",
    "                if chunk.choices[0].delta.content\n",
    "            )\n",
    "            \n",
    "            return full_response.strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in query generation: {e}\")\n",
    "            query='no llm'\n",
    "            return query  # Fallback to original query\n",
    "\n",
    "    def should_use_original(self, query):\n",
    "        \"\"\"\n",
    "        Determines if the original query should be used for similarity retrieval.\n",
    "        \n",
    "        Parameters:\n",
    "            query (str): The input query.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if original query should be used, False otherwise.\n",
    "        \"\"\"\n",
    "        # Add more sophisticated logic if needed\n",
    "        return len(query.strip()) > 3  # Use original query if longer than 3 characters\n",
    "    \n",
    "         \n",
    "    def search_vectordb_chat(self, search_field, search_value, k=3):\n",
    "        \"\"\"Searches the vector database based on user-specified field and value.\"\"\"\n",
    "        collection = self.vectordb._collection\n",
    "        search_field = \"ID\"\n",
    "        search_value = int(search_value)  # Cast to int for consistency\n",
    "        # Fetch the raw results from the database\n",
    "        results = collection.get(where={search_field: search_value})\n",
    "\n",
    "        if results['documents']:  # Check if there are any documents\n",
    "            # Combine documents and metadata into a list of tuples for sorting\n",
    "            document_metadata_pairs = [\n",
    "                (results['documents'][i], results['metadatas'][i])\n",
    "                for i in range(len(results['documents']))\n",
    "            ]\n",
    "\n",
    "            # Filter and sort the combined list by chunk_id\n",
    "            filtered_sorted_chunks = sorted(\n",
    "                [ (content, metadata) for content, metadata in document_metadata_pairs \n",
    "                  if metadata.get('doc_id') == search_value ],\n",
    "                key=lambda x: x[1].get('chunk_id')  # Sort by chunk_id\n",
    "            )\n",
    "\n",
    "            # Prepare the formatted text for the output\n",
    "            formatted_texts = []\n",
    "            for doc_number, (content, metadata) in enumerate(filtered_sorted_chunks, start=1):\n",
    "                chunk_id = metadata.get('chunk_id')\n",
    "                doc_text = (\n",
    "                    f\"**Document {doc_number}: Chunk ID: {chunk_id}, Metadata: {metadata}**\\n\"\n",
    "                    f\"Content: {content}\\n\"\n",
    "                )\n",
    "                formatted_texts.append(doc_text.strip())  # Clean up text\n",
    "\n",
    "            # Join the list into a single string\n",
    "            final_text = \"\\n\\n\".join(formatted_texts).strip()\n",
    "            # Return the formatted text\n",
    "            return final_text\n",
    "        else:\n",
    "            return \"No documents found.\"    \n",
    "        \n",
    "\n",
    "def on_input_change(event):\n",
    "    # Check if the user pressed ENTER (key code 13)\n",
    "    if event.new and event.new[-1] == '\\n':  # Check if the last character is a newline (ENTER)\n",
    "        handle_input2(vectordb, \n",
    "                      embedding, \n",
    "                      llm, \n",
    "                      retriever, \n",
    "                      event.new[:-1],\n",
    "                      selected_method_value=selected_method.value,\n",
    "                      k_value=k_value_widget.value)  # Exclude the newline character\n",
    "        \n",
    "        \n",
    "def load_css_file(file_path):\n",
    "    \"\"\"\n",
    "    Safely load CSS file with error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"CSS file not found: {file_path}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSS file: {e}\")\n",
    "        return \"\"\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7ef748-65ed-442a-8b75-c4c5eb0448e6",
   "metadata": {},
   "source": [
    "# Required to Run: Alternative Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9229f8b-46a5-4dd8-9012-2706b695731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import OpenAI\n",
    "# Set up environment\n",
    "# Set environment variable\n",
    "\n",
    "# Initialize client with local LM Studio endpoint\n",
    "llm = OpenAI(\n",
    "    base_url=\"http://localhost:1238/v1\", \n",
    "    api_key=\"lm-studio\"\n",
    ")\n",
    "\n",
    "#llm  = OpenAI(base_url=\"http://localhost:1238/v1\", api_key=\"lm-studio\")\n",
    "#llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# we instantiated the retreiever above\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), \n",
    "    llm=llm\n",
    ")\n",
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8de174-2c68-487b-ba6c-316fb118ae95",
   "metadata": {},
   "source": [
    "# PDF retrieval with keeping metdata as citation with contents in text\n",
    "# PANEL DASHBOARD\n",
    "## 1st Based on  client.chat.completions.create  : \n",
    "    * Browser TAB\n",
    "    * Prompt Input\n",
    "    * Query\n",
    "    * Response\n",
    "    * Whole message sent to Model\n",
    "## Not tested: Chose Retrieval: Standard or Multi (langChain): line 76\n",
    "    * retriever_from_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d20d8-c2c2-4346-9283-d5d4e2c7ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#own class\n",
    "#from assets.class_retrieval import DocumentRetriever\n",
    "#end own class\n",
    "import panel as pn\n",
    "from openai import OpenAI\n",
    "#from langchain_openai import OpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "#from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "import re\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "%run assets/func_inputoutput.py\n",
    "\n",
    "# Disable warnings and proxy settings\n",
    "os.environ['NO_PROXY'] = 'localhost,127.0.0.1'\n",
    "urllib3.disable_warnings()\n",
    "urllib3.util.connection.is_connection_dropped = lambda conn: False\n",
    "\n",
    "# Configure requests to ignore proxies\n",
    "import requests\n",
    "requests.Session().trust_env = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the template for the prompt t1\n",
    "template1 = \"\"\"\n",
    "Use the following retrieved documents and the previous conversation to answer the query. Incorporate your own knowledge and reasoning as an AI assistant:\n",
    "Previous Conversation: {history}\n",
    "Query: {query}\n",
    "Retrieved Documents: {retrieved_docs}\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "\n",
    "# Define the template for the prompt t2\n",
    "template = \"\"\"\n",
    "Use the following Retrieved Documents and the previous conversation to answer the query. Incorporate your own knowledge and reasoning as an AI assistant.  Keep the content together with document and chunk numbers at the end of sentences in brackets. If content from different documents is combined into a new sentence, place the document and chunk numbers at the end of the sentence, separated by semicolons.\n",
    "Previous Conversation: {history}\n",
    "Query: {query}\n",
    "Retrieved Documents: {retrieved_docs}\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "\n",
    "template2 = \"\"\"\n",
    "CRITICAL CITATION INSTRUCTIONS:\n",
    "1. MANDATORY: Cite the EXACT Document and Chunk number for EVERY piece of information retrieved from Retrieved Documents\n",
    "2. Format citations EXACTLY like this: \n",
    "   - Single source: \"(Document Number, Chunk Number)\"\n",
    "   - Multiple sources: \"(Document Number, Chunk Number; Document Number, Chunk Number)\"\n",
    "3. Place citations IMMEDIATELY after the relevant information, inside parentheses\n",
    "4. NEVER omit citations\n",
    "5. If rephrasing content, still include original source citation\n",
    "\n",
    "Use the following Retrieved Documents and the previous conversation to answer the query:\n",
    "\n",
    "Previous Conversation: {history}\n",
    "Query: {query}\n",
    "Retrieved Documents: {retrieved_docs}\n",
    "\n",
    "RESPONSE GUIDELINES:\n",
    "- Create a cohesive, well-structured paragraph\n",
    "- Ensure EVERY fact is explicitly cited\n",
    "- Maintain scientific rigor and precision\n",
    "- Include citations WITHOUT FAIL\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "# Create the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"history\", \"query\", \"retrieved_docs\", \"answer\"],  # Removed 'context'\n",
    "    template=template\n",
    ")\n",
    "\n",
    "\n",
    "history = []\n",
    "\n",
    "# Point to the local OpenAI server\n",
    "client = OpenAI(base_url=\"http://localhost:1238/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "\n",
    "which_retriever='Standard'\n",
    "#which_retriever='Standard'\n",
    "\n",
    "if which_retriever =='Multi':\n",
    "    retriever=retriever_from_llm\n",
    "else:\n",
    "    # Set up the vector retriever\n",
    "    retriever = vectordb.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Define a function to handle the user's input\n",
    "def handle_input2(vectordb, embedding, llm, retriever, input_value=None, selected_method_value=None, k_value=None): # Pass as arguments\n",
    "    global is_first_run  # Access global variables\n",
    "    wrap_width = 100\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if is_first_run:\n",
    "        query = \"Hello, introduce yourself to someone opening this program for the first time. Be concise. Do not add citations to your first response.\"\n",
    "        \n",
    "        similar_docs=[]        \n",
    "        output_box_retrievalTime.object = f\"Processing Retrieval {is_first_run}\"\n",
    "    else:\n",
    "        output_box_retrievalTime.object = f\"Processing Retrieval:\"\n",
    "        output_box_gpt_responseTime = f\"Waiting for GPT\"\n",
    "        #start_timere = time.time()\n",
    "        query = input_box.value.strip()\n",
    "\n",
    "    #history.append({\"role\": \"user\", \"content\": query)# + \" Database content: [\" + some_context + \"]\"})\n",
    "\n",
    "    if query:\n",
    "        managed_history = manage_conversation_history(history)\n",
    "        #vector_db_query = \"Image normalization methods for analyzing brain metabolism in different brain regions.\"  # Use the concise prompt\n",
    "        \n",
    "        selected_method =selected_method_value #'combined' #'keywords' #llm, combined \n",
    "        k=k_value #12\n",
    "        \n",
    "        retrieved_docs = \"\"        \n",
    "        retriever = DocumentRetriever(vectordb)  # Pass the chosen LLM\n",
    "        retrieved_docs, used_query, method_retquery = retriever.retrieve_documents(query, is_first_run, k=k, method=selected_method)\n",
    "                \n",
    "        used_query=f\"**Used Retrieval Query:**  \\n {used_query}\\n\"       \n",
    "        \n",
    "        output_box3.object=used_query\n",
    "       \n",
    "        \n",
    "        if '{no history}' not in query:\n",
    "            prompt = prompt_template.format(\n",
    "                history=managed_history,\n",
    "                query=query,\n",
    "                retrieved_docs=retrieved_docs,\n",
    "                answer=\"\"\n",
    "            )\n",
    "        else:\n",
    "            prompt = prompt_template.format(\n",
    "                history=\"\",\n",
    "                query=query,\n",
    "                retrieved_docs=retrieved_docs,\n",
    "                answer=\"\"\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        output_box2.object = prompt\n",
    "        \n",
    "        end_timere = time.time()\n",
    "        elapsed_timere = end_timere - start_time\n",
    "        \n",
    "        retrieved_tokens=word_count(retrieved_docs)\n",
    "        output_box_retrievalTime.object = f\"Time for Retrieval with: {elapsed_timere:.2f} \\\n",
    "        seconds | retrieved_docs_length {retrieved_tokens} | selected_method {selected_method}\\n | k {k}\"\n",
    "       \n",
    "\n",
    "        # Display formatted messages in output box\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"\"\"You are a scientific document analysis AI. \"\"\" },\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ] \n",
    "        \n",
    " \n",
    "                    \n",
    "        \n",
    "        completion = client.chat.completions.create(\n",
    "            #model=\"LLMA/Meta-Llama-3-8B\",  # Correct model name/path here!\n",
    "            model=\"lmstudio/Meta-Llama-3.1\",\n",
    "                      \n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "            stream=True,\n",
    "        )\n",
    "\n",
    "        new_message = {\"role\": \"assistant\", \"content\": \"\"}\n",
    "        full_response = \"\"\n",
    "        for chunk in completion:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                new_message[\"content\"] += chunk.choices[0].delta.content\n",
    "                full_response += chunk.choices[0].delta.content\n",
    "\n",
    "\n",
    "        # Combine the summary and LLM response in the displayed output\n",
    "        final_output = f\"Final Answer:\\n{full_response}\"  # Adjust formatting as needed\n",
    "        formatted_text = f\"**Query:**  \\n{query}\\n\\n**Final Answer (with Summaries):**  \\n{final_output}\"\n",
    "        #not tested: formatted_text = f\"<div style='white-space: normal; word-wrap: break-word; overflow-wrap: break-word; font-family: Arial, sans-serif; font-size: 12px;'>\\\n",
    "        #                **Query:**<br>{query}<br><br>**Final Answer (with Summaries):**<br>{final_output}</div>\"\n",
    "        output_box.object = formatted_text  # Update output box  \n",
    "        \n",
    "        message_tokens=word_count(messages)\n",
    "        history_tokens=word_count(managed_history) \n",
    "        \n",
    "        history.append(new_message)   \n",
    "        \n",
    "        input_box.value = \"\"        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        output_box_responseTime.object = f\"Time taken for Model: {elapsed_time:.2f} seconds | message_length: {message_tokens} | history {history_tokens}\"\n",
    "        \n",
    "    is_first_run = False\n",
    "\n",
    "\n",
    "# Define a function to handle user input changes\n",
    "def handle_input_change(event):\n",
    "    handle_input2(vectordb, embedding, llm, retriever, input_value=input_box.value,\n",
    "                   selected_method_value=selected_method.value,\n",
    "                   k_value=k_value_widget.value)\n",
    "\n",
    "# Apply debounce to input handling\n",
    "@debounce(wait=0.1)  # 500 ms delay\n",
    "def handle_input_debounced(event):\n",
    "    handle_input2(vectordb, embedding, llm, retriever,input_value=input_box.value, \n",
    "                   selected_method_value=selected_method.value,\n",
    "                   k_value=k_value_widget.value)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Specify the path to your CSS file\n",
    "css_path = os.path.join('assets', 'stylesdeep.css')\n",
    "\n",
    "# Load the CSS file and pass it to Panel's extension\n",
    "with open(css_path, 'r') as f:\n",
    "    custom_css = f.read()\n",
    "\n",
    "# Load the CSS directly in the extension\n",
    "pn.extension(raw_css=[custom_css])\n",
    "\n",
    "\n",
    "\n",
    "selected_method = pn.widgets.Select(\n",
    "    name='Select Method of Keyword Generation', \n",
    "    options=['combined', 'keywords', 'llm'], \n",
    "    value='combined',                                    \n",
    "    css_classes=[\"outer-style\"]        \n",
    ") \n",
    "\n",
    "# Create the slider widget for k_value\n",
    "k_value_widget = pn.widgets.IntSlider(\n",
    "    start=1,          # Minimum value\n",
    "    end=100,          # Maximum value\n",
    "    value=10,         # Default value\n",
    "    step=1,           # Step size\n",
    "    name='Select K Number of Retrievals'\n",
    "    #css_classes=[\"k-value-widget\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Improved Input Box Styling\n",
    "input_box = pn.widgets.TextAreaInput(\n",
    "    width=None, \n",
    "    height=300,  # Ensure both boxes have the same height\n",
    "    placeholder=\"Enter your query here...Execute with ENTER\",\n",
    "    css_classes=[\"input-box\"]\n",
    ")\n",
    "\n",
    "# Loading Indicator Styling\n",
    "loading_indicator = pn.indicators.LoadingSpinner(\n",
    "    value=False, \n",
    "    width=20, \n",
    "    height=20,\n",
    "    css_classes=[\"loading-indicator\"]\n",
    ")\n",
    "\n",
    "# Output Box Styling\n",
    "output_box = pn.pane.Markdown(\n",
    "    width=None,\n",
    "    height=300,  # Ensure both boxes have the same height\n",
    "    css_classes=[\"output-box\"]\n",
    ")\n",
    "\n",
    "output_box3 = pn.pane.HTML(\n",
    "    width=None,\n",
    "    height=100,\n",
    "    css_classes=[\"output-box3\"]  \n",
    "\n",
    ")\n",
    "# Apply similar styling to output_box2\n",
    "output_box2 = pn.pane.Markdown(\n",
    "    width=None,\n",
    "    height=300,\n",
    "    css_classes=[\"output-box2\"]\n",
    ")\n",
    "\n",
    "# Time Output Boxes with more subtle styling\n",
    "output_box_retrievalTime = pn.pane.Markdown(css_classes=[\"output-box-retrieval-time\"])\n",
    "\n",
    "output_box_responseTime = pn.pane.Markdown(css_classes=[\"output-box-response-time\"])\n",
    "\n",
    "def read_values(event):\n",
    "    method = selected_method.value  # Get the selected method\n",
    "    k_value = k_value_widget.value   # Get the K value\n",
    "# Watch for changes in the input box value\n",
    "#input_box.param.watch(lambda event: handle_input2(vectordb, embedding, llm, retriever), 'value')  # Pass arguments here\n",
    "input_box.param.watch(on_input_change, 'value')\n",
    "selected_method.param.watch(read_values, 'value')\n",
    "k_value_widget.param.watch(read_values, 'value')\n",
    "#input_box.param.watch(lambda event: handle_input2(vectordb, embedding, llm, retriever) if event.new.endswith('\\n') else None, 'value')\n",
    "#input_box.param.watch(handle_input_debounced, 'value')\n",
    "# Create a variable to track if it's the first run\n",
    "is_first_run = True\n",
    "if is_first_run:\n",
    "        # Call handle_input2()\n",
    "        handle_input2(vectordb, embedding, llm, retriever, input_value=input_box.value,\n",
    "                   selected_method_value=selected_method.value,\n",
    "                   k_value=k_value_widget.value)\n",
    "        is_first_run = False\n",
    "\n",
    "\n",
    "       \n",
    "        \n",
    "        \n",
    "# Create the dashboard layout with improved responsiveness\n",
    "dashboard_layout = pn.Column(\n",
    "    pn.Row(\n",
    "        pn.pane.Markdown(\n",
    "             \"<h1 style='font-size: 14px; color: #2c3e50;'>ChatGPT-like Conversation: RAG</h1>\",\n",
    "            css_classes=[\"markdown-header\"]\n",
    "        ),\n",
    "        #loading_indicator,\n",
    "        selected_method,\n",
    "        k_value_widget,\n",
    "        sizing_mode='stretch_width',\n",
    "        align='start',\n",
    "        height=50  # Set a fixed, smaller height\n",
    "    ),\n",
    "    \n",
    "    # Wrapper Row with explicit styling\n",
    "    pn.Row(\n",
    "        pn.Column(\n",
    "            input_box,\n",
    "            width_policy='max',\n",
    "            sizing_mode='stretch_width',\n",
    "            styles={\n",
    "                'flex': '1'  # 1/3 of the row\n",
    "            }\n",
    "        ),\n",
    "        pn.Column(\n",
    "            output_box,\n",
    "            width_policy='max',\n",
    "            sizing_mode='stretch_width',\n",
    "            styles={\n",
    "                'flex': '2'# 2/3 of the row\n",
    "            }\n",
    "        ),\n",
    "        align='start',  # Align items at the top\n",
    "        sizing_mode='stretch_width',\n",
    "        css_classes=[\"dashboard-layout\"]\n",
    "    ),\n",
    "    \n",
    "    # Compact Time Output Row\n",
    "    pn.Row(\n",
    "        pn.Column(\n",
    "            output_box_retrievalTime, \n",
    "            width_policy='max',\n",
    "            sizing_mode='stretch_width'\n",
    "        ), \n",
    "        pn.Column(\n",
    "            output_box_responseTime,\n",
    "            width_policy='max', \n",
    "            sizing_mode='stretch_width'\n",
    "        ),\n",
    "        sizing_mode='stretch_width',\n",
    "        css_classes=[\"dashboard-layout\"]\n",
    "    ),\n",
    "    # Additional Output Row\n",
    "    pn.Row(\n",
    "        output_box3,\n",
    "        sizing_mode='stretch_width'\n",
    "    ),    \n",
    "    # Additional Output Row\n",
    "    pn.Row(\n",
    "        output_box2,\n",
    "        sizing_mode='stretch_width'\n",
    "    ),\n",
    "    \n",
    "    sizing_mode='stretch_width',\n",
    "    css_classes=[\"dashboard-layout\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Create a dashboard object using the layout\n",
    "dashboard = pn.panel(\n",
    "    dashboard_layout, \n",
    "    sizing_mode='stretch_both'  # Ensure dashboard itself stretches\n",
    ")\n",
    "\n",
    "# Display the dashboard in a new browser tab\n",
    "dashboard.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09696fb9-d8d4-4527-a538-2184ca3219bc",
   "metadata": {},
   "source": [
    "# Stop: Next similar but more general chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f8b717-2eda-49a2-92e1-3373fcd4fdd8",
   "metadata": {},
   "source": [
    "## 2nd Based on  client.chat.completions.create  : \n",
    "    * Browser TAB\n",
    "    * Prompt Input\n",
    "    * Query\n",
    "    * Response\n",
    "    * Whole message sent to Model\n",
    "## Chose Retrieval: Standard or Multi (langChain): line 32\n",
    " * overall the responses are quite slow, because of retrieval of useful text snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d6e95-62c3-47d8-97a7-5ed54a0d3ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "print(pn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3811b2-7699-481c-a187-7260251acd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd\n",
    "import panel as pn\n",
    "from openai import OpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "#from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "import re\n",
    "import time\n",
    "\n",
    "\n",
    "# Define the template for the prompt\n",
    "template = \"\"\"\n",
    "Use the following context from the vector database and the previous conversation to answer the query. Incorporate your own knowledge and reasoning as an AI assistant:\n",
    "Previous Conversation: {history}\n",
    "Context: {context}\n",
    "Query: {query}\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Create the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"history\", \"context\", \"query\", \"answer\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "history = []\n",
    "\n",
    "# Point to the local OpenAI server\n",
    "client = OpenAI(base_url=\"http://localhost:1238/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "\n",
    "which_retriever='Multi'\n",
    "#which_retriever='Standard'\n",
    "\n",
    "if which_retriever =='Multi':\n",
    "    retriever=retriever_from_llm\n",
    "else:\n",
    "    # Set up the vector retriever\n",
    "    retriever = vectordb.as_retriever()\n",
    "\n",
    "    \n",
    "# Define a function to handle the user's input\n",
    "def handle_input2():\n",
    "    global is_first_run\n",
    "    wrap_width = 100\n",
    "\n",
    "    if is_first_run:\n",
    "        query = \"Hello, introduce yourself to someone opening this program for the first time. Be concise\"\n",
    "        is_first_run = False\n",
    "        some_context = \"\"\n",
    "    else:\n",
    "        output_box_retrievalTime.object = f\"Processing Retrieval {which_retriever}\"\n",
    "        output_box_gpt_responseTime = f\"Waiting for GPT\"\n",
    "        start_time = time.time()\n",
    "        query = input_box.value.strip()\n",
    "        search_results = retriever.get_relevant_documents(query)\n",
    "        #search_results = retriever_from_llm.get_relevant_documents(query)\n",
    "        some_context = \"\"\n",
    "        for result in search_results:\n",
    "            some_context += result.page_content + \"'Title-- '\"+result.metadata['Title']+\"--EndTitle | \\n\\n\"\n",
    "        \n",
    "        end_time = time.time()\n",
    "        # Calculate the elapsed time\n",
    "        elapsed_time = end_time - start_time\n",
    "        output_box_retrievalTime.object = f\"Time taken for Retrieval with: {elapsed_time:.2f} seconds\"\n",
    "    #history.append({\"role\": \"user\", \"content\": query)# + \" Database content: [\" + some_context + \"]\"})\n",
    "\n",
    "    if query:\n",
    "        # Generate the prompt using the template and input values\n",
    "        prompt = prompt_template.format(history=history, context=some_context, query=query, answer=\"\")\n",
    "        #prompt = prompt_template.format(history=history, answer=\"\")\n",
    "        output_box2.object=prompt\n",
    "        start_time = time.time()\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"LLMA/Meta-Llama-3-8B\",\n",
    "            messages=history + [{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,\n",
    "            stream=True,\n",
    "        )\n",
    "\n",
    "        \n",
    "        new_message = {\"role\": \"assistant\", \"content\": \"\"}\n",
    "\n",
    "        full_response = \"\"\n",
    "        for chunk in completion:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                new_message[\"content\"] += chunk.choices[0].delta.content\n",
    "                full_response += (chunk.choices[0].delta.content)\n",
    "\n",
    "        formatted_text = f\"<pre style='white-space: pre-wrap; width: {wrap_width}ch; font-family: Arial, sans-serif; font-size: 12px;'>**Query:**\\n{query}\\n\\n**Final Answer:**\\n{full_response}</pre>\"\n",
    "        output_box.object = formatted_text\n",
    "\n",
    "        history.append(new_message)\n",
    "        \n",
    "        input_box.value = \"\"\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        output_box_responseTime.object = f\"Time taken for Model: {elapsed_time:.2f} seconds\"\n",
    "\n",
    "\n",
    "# Define a function to handle user input changes\n",
    "def handle_input_change(event):\n",
    "    handle_input2()\n",
    "\n",
    "# Load the Panel extension\n",
    "pn.extension()\n",
    "\n",
    "# Create an input box\n",
    "input_box = pn.widgets.TextAreaInput(width=400, height=100, styles={'overflow-y': 'scroll', 'text-align': 'center'})\n",
    "# Create the output box\n",
    "output_box = pn.pane.Markdown(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    styles={\n",
    "        'overflow-y': 'scroll',\n",
    "        'color': 'black',                   # Text color\n",
    "        'background-color': '#f0f0f0',      # Background color (light gray)\n",
    "        'font-size': '14px',                # Font size\n",
    "        'font-family': 'Courier New',       # Font family\n",
    "        'padding': '5px',                   # Padding inside the pane\n",
    "        'border': '1px solid red',          # Border around the pane\n",
    "        'text-align': 'center'              # Center the text\n",
    "    }\n",
    ")\n",
    "\n",
    "output_box2 = pn.pane.Markdown(\n",
    "    height=400,\n",
    "    styles={\n",
    "        'overflow-y': 'scroll',\n",
    "        'overflow-x': 'scroll',\n",
    "        'color': 'black',        # Text color\n",
    "        'background-color': '#f0f0f4',  # Background color\n",
    "        'font-size': '14px',     # Font size\n",
    "        'font-family': 'Courier New',  # Font family\n",
    "        'padding': '5px',        # Padding inside the pane\n",
    "        'border': '1px solid blue'  # Border around the pane\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "output_box_retrievalTime = pn.pane.Markdown()\n",
    "output_box_responseTime = pn.pane.Markdown()\n",
    "# Watch for changes in the input box value\n",
    "input_box.param.watch(handle_input_change, 'value')\n",
    "\n",
    "# Create a variable to track if it's the first run\n",
    "is_first_run = True\n",
    "if is_first_run:\n",
    "        # Call handle_input2()\n",
    "        handle_input2()\n",
    "        is_first_run = False\n",
    "\n",
    "         \n",
    "# Create the dashboard layout using Panel\n",
    "dashboard_layout = pn.Column(\n",
    "    pn.pane.Markdown(\"# ChatGPT-like Conversation: RAG\"),\n",
    "    pn.Spacer(height=20),\n",
    "    pn.Row(input_box, output_box),\n",
    "    pn.Row(pn.Spacer(height=20)),\n",
    "    pn.Row(output_box_retrievalTime, output_box_responseTime),\n",
    "    pn.Row(output_box2),\n",
    "    name=\"ChatGPT Dashboard\"\n",
    ")\n",
    "\n",
    "# Create a dashboard object using the layout\n",
    "dashboard = pn.panel(dashboard_layout)\n",
    "\n",
    "# Display the dashboard in a new browser tab\n",
    "dashboard.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f50ae-1a12-4aae-aa90-7f5f947c971e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a3780-68d2-4fea-8fc5-e2984cdfd73f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fccb34c-790d-493f-a74f-5b2a12ba55c9",
   "metadata": {},
   "source": [
    "# Alternative Version: based on QA CHAIN with BROWSER TAB - \n",
    " * output still not consitent and short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac895296-a285-4bbb-b26d-5ae9eebc19fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "import textwrap\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "# Create a RAG model\n",
    "llm = OpenAI(base_url=\"http://localhost:1238/v1\", api_key=\"lm-studio\")\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "# Define a custom prompt template\n",
    "template = \"\"\"\n",
    "Use the following context from the vector database and the previous conversation to answer the query. Incorporate your own knowledge and reasoning as an AI assistant:\n",
    "\n",
    "Previous Conversation: {history}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables=[\"history\", \"context\", \"query\"], template=template)\n",
    "\n",
    "# Create a custom chain\n",
    "qa_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "\n",
    "# Load the Panel extension\n",
    "pn.extension()\n",
    "\n",
    "import textwrap\n",
    "import panel as pn\n",
    "# Create an input box\n",
    "input_box = pn.widgets.TextAreaInput()\n",
    "# Create the output box\n",
    "output_box = pn.pane.Markdown()\n",
    "# Create a history list to store the conversation\n",
    "history = []\n",
    "# Define a function to handle the user's input\n",
    "def handle_input():\n",
    "    query = input_box.value.strip()\n",
    "    if query:\n",
    "        # Add user input to the history\n",
    "        #history.append({\"role\": \"user\", \"content\": query})\n",
    "        # Process the query and generate a response\n",
    "        context = retriever.get_relevant_documents(query)\n",
    "        result_str = qa_chain.run(\n",
    "            history=\"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in history]),\n",
    "            query=query,\n",
    "            #max_tokens=8096,\n",
    "            max_tokens=-1,\n",
    "            context=\"\".join([doc.page_content for doc in context])\n",
    "        )\n",
    "        final_answer_start = result_str.find(\"Final Answer:\")\n",
    "        if final_answer_start != -1:\n",
    "            final_answer = result_str[final_answer_start + len(\"Final Answer:\"):].strip()\n",
    "            output_box.object = f\"**Query:**\\n{query}\\n\\n**Final Answer:**\\n{textwrap.fill(final_answer, width=80)}\"\n",
    "        else:\n",
    "            final_answer=result_str\n",
    "            output_box.object = f\"**Query:**\\n{query}\\n\\n**Answer:**\\n{textwrap.fill(result_str, width=80)}\"\n",
    "        \n",
    "        # Add assistant response to the history\n",
    "        history.append({\"role\": \"assistant\", \"content\": final_answer})\n",
    "        history.append({\"role\": \"user\", \"content\": query})\n",
    "        # Clear the input box\n",
    "        input_box.value = \"\"\n",
    "        \n",
    "        \n",
    "# Watch for changes in the input box value\n",
    "input_box.param.watch(lambda event: handle_input(), 'value')\n",
    "# Create the dashboard layout using Panel\n",
    "dashboard_layout = pn.Column(\n",
    "    pn.pane.Markdown(\"# ChatGPT-like Conversation\"),\n",
    "    pn.Spacer(height=20),\n",
    "    pn.Row(input_box, output_box),\n",
    "    name=\"ChatGPT Dashboard\"\n",
    ")\n",
    "# Create a dashboard object using the layout\n",
    "dashboard = pn.panel(dashboard_layout)\n",
    "# Display the dashboard in a new browser tab\n",
    "dashboard.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c0f4f-c064-4988-8e03-8d804d71717c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
