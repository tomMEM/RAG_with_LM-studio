{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a84502f6-d41e-447c-b9e2-f2cd4d9958f1",
   "metadata": {},
   "source": [
    " * config.json  with \n",
    "```text\n",
    "{\n",
    "  \"GEMINI_API_KEY\": \"Your Key\",\n",
    "  \"KIMI_API_KEY\":  \"Your Key\",\n",
    "  \"OPENAI_API_KEY:\"OPENAI_API_KEY\",\n",
    "  \"HTTP_PROXY_URL\": \"http://localhost:port\"\n",
    "} \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15a1977-3df0-4319-8d44-a0f446846756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# funct\n",
    "import os\n",
    "import gradio as gr\n",
    "import traceback\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "import socket\n",
    "\n",
    "# --- Proxy check ---\n",
    "def is_proxy_available(url):\n",
    "    \"\"\"Checks if a proxy is available by trying to connect to its host and port.\"\"\"\n",
    "    try:\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.settimeout(1)\n",
    "        address = (url[\"host\"], url[\"port\"])\n",
    "        result = sock.connect_ex(address)\n",
    "        if result == 0:\n",
    "            return True\n",
    "    except socket.error as err:\n",
    "        print(f\"Socket error while checking proxy: {err}\")\n",
    "    except KeyError:\n",
    "        print(f\"Invalid URL dictionary passed to is_proxy_available: {url}\")\n",
    "    finally:\n",
    "        sock.close()\n",
    "    return False\n",
    "\n",
    "# --- Global Configuration ---\n",
    "CONFIG = {}\n",
    "\n",
    "# --- Global Variables ---\n",
    "kimi_client = None\n",
    "gemini_model = None\n",
    "openai_client = None\n",
    "current_model_technical_name = None\n",
    "gemini_configured = False\n",
    "kimi_configured = False\n",
    "openai_configured = False\n",
    "\n",
    "# --- Model List ---\n",
    "AVAILABLE_MODELS = {\n",
    "    # Kimi Models\n",
    "    \"Kimi (8k context)\": \"moonshot-v1-8k\",\n",
    "    \"Kimi (32k context)\": \"moonshot-v1-32k\",\n",
    "    \"Kimi (128k context)\": \"moonshot-v1-128k\",\n",
    "\n",
    "    # Google Gemini Models\n",
    "    \"Gemini 1.5 Flash\": \"gemini-1.5-flash-latest\",\n",
    "    \"Gemini 2.5 Flash\": \"gemini-2.5-flash-latest\",\n",
    "    \"Gemini 1.5 Pro\": \"gemini-1.5-pro-latest\",\n",
    "    \"Gemini 2.5 Pro\": \"gemini-2.5-pro-latest\",\n",
    "\n",
    "    # OpenAI Models\n",
    "    \"OpenAI GPT-4.1\": \"gpt-4.1\",\n",
    "    \"OpenAI GPT-4o Mini\": \"gpt-4o-mini\",\n",
    "    \"OpenAI DALL·E 3\": \"dall-e-3\",\n",
    "}\n",
    "\n",
    "DEFAULT_MODEL_DISPLAY_NAME = \"Kimi (32k context)\"\n",
    "\n",
    "# --- Load Config ---\n",
    "def load_and_configure():\n",
    "    global CONFIG\n",
    "    try:\n",
    "        config_path = os.path.join(os.getcwd(), 'config.json')\n",
    "        print(f\"Attempting to load configuration from: {config_path}\")\n",
    "        if os.path.exists(config_path):\n",
    "            with open(config_path, 'r') as f:\n",
    "                CONFIG = json.load(f)\n",
    "            print(\"Successfully loaded settings from config.json\")\n",
    "        else:\n",
    "            print(\"WARNING: config.json not found.\")\n",
    "            CONFIG = {}\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not read or parse config.json: {e}\")\n",
    "        CONFIG = {}\n",
    "\n",
    "    CONFIG['GEMINI_API_KEY'] = os.environ.get(\"GEMINI_API_KEY\", CONFIG.get('GEMINI_API_KEY'))\n",
    "    CONFIG['KIMI_API_KEY'] = os.environ.get(\"KIMI_API_KEY\", CONFIG.get('KIMI_API_KEY'))\n",
    "    CONFIG['OPENAI_API_KEY'] = os.environ.get(\"OPENAI_API_KEY\", CONFIG.get('OPENAI_API_KEY'))\n",
    "    CONFIG['HTTP_PROXY_URL'] = os.environ.get(\"HTTP_PROXY_URL\", CONFIG.get('HTTP_PROXY_URL'))\n",
    "\n",
    "    proxy_url = CONFIG.get(\"HTTP_PROXY_URL\")\n",
    "    if proxy_url:\n",
    "        proxy_parts = proxy_url.split('://')[-1].split(':')\n",
    "        host = proxy_parts[0]\n",
    "        port = int(proxy_parts[-1])\n",
    "        if is_proxy_available({\"host\": host, \"port\": port}):\n",
    "            os.environ['http_proxy'] = proxy_url\n",
    "            os.environ['https_proxy'] = proxy_url\n",
    "            print(f\"Proxy set globally: {proxy_url}\")\n",
    "        else:\n",
    "            print(\"Proxy not available. Using direct connection.\")\n",
    "            os.environ.pop('http_proxy', None)\n",
    "            os.environ.pop('https_proxy', None)\n",
    "    else:\n",
    "        print(\"No proxy configured.\")\n",
    "        os.environ.pop('http_proxy', None)\n",
    "        os.environ.pop('https_proxy', None)\n",
    "\n",
    "    current_no_proxy = os.environ.get('NO_PROXY', '')\n",
    "    additional_no_proxy_hosts = ['localhost', '127.0.0.1', '0.0.0.0']\n",
    "    new_no_proxy_parts = [host for host in current_no_proxy.split(',') if host.strip()]\n",
    "    for host in additional_no_proxy_hosts:\n",
    "        if host not in new_no_proxy_parts:\n",
    "            new_no_proxy_parts.append(host)\n",
    "    os.environ['NO_PROXY'] = ','.join(new_no_proxy_parts)\n",
    "    print(f\"NO_PROXY set for Gradio: {os.environ.get('NO_PROXY')}\")\n",
    "\n",
    "# --- Initialize APIs ---\n",
    "def initialize_api_clients():\n",
    "    global kimi_client, gemini_configured, kimi_configured, openai_client, openai_configured\n",
    "\n",
    "    # Gemini\n",
    "    gemini_api_key = CONFIG.get('GEMINI_API_KEY')\n",
    "    if gemini_api_key:\n",
    "        try:\n",
    "            genai.configure(api_key=gemini_api_key)\n",
    "            gemini_configured = True\n",
    "            print(\"Google Gemini API configured.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Gemini API: {e}\")\n",
    "    else:\n",
    "        print(\"GEMINI_API_KEY not found.\")\n",
    "\n",
    "    # Kimi\n",
    "    kimi_api_key = CONFIG.get('KIMI_API_KEY')\n",
    "    if kimi_api_key:\n",
    "        try:\n",
    "            kimi_client = OpenAI(api_key=kimi_api_key, base_url=\"https://api.moonshot.cn/v1\")\n",
    "            kimi_configured = True\n",
    "            print(\"Kimi API configured.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Kimi API: {e}\")\n",
    "    else:\n",
    "        print(\"KIMI_API_KEY not found.\")\n",
    "\n",
    "    # OpenAI\n",
    "    openai_api_key = CONFIG.get('OPENAI_API_KEY')\n",
    "    if openai_api_key:\n",
    "        try:\n",
    "            openai_client = OpenAI(api_key=openai_api_key)\n",
    "            openai_configured = True\n",
    "            print(\"OpenAI API configured.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: OpenAI API: {e}\")\n",
    "    else:\n",
    "        print(\"OPENAI_API_KEY not found.\")\n",
    "\n",
    "# --- Chat Responders ---\n",
    "def gemini_chat_responder(user_message, chat_history):\n",
    "    global gemini_model, current_model_technical_name\n",
    "    if gemini_model is None or gemini_model.model_name != f\"models/{current_model_technical_name}\":\n",
    "        gemini_model = genai.GenerativeModel(current_model_technical_name)\n",
    "    gemini_history = [{'role': 'user' if item[0] else 'model', 'parts': [item[0] if item[0] else item[1]]} for item in chat_history]\n",
    "    try:\n",
    "        chat_session = gemini_model.start_chat(history=gemini_history)\n",
    "        response = chat_session.send_message(user_message)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        traceback.print_exc(); return f\"Gemini error: {e}\"\n",
    "\n",
    "def kimi_chat_responder(user_message, chat_history):\n",
    "    messages = [{\"role\": \"system\", \"content\": \"你是 Kimi，由 Moonshot AI 出品的超长上下文人工智能助手。\"}]\n",
    "    for user_msg, ai_msg in chat_history:\n",
    "        messages.extend([{'role': 'user', 'content': user_msg}, {'role': 'assistant', 'content': ai_msg}])\n",
    "    messages.append({'role': 'user', 'content': user_message})\n",
    "    try:\n",
    "        response = kimi_client.chat.completions.create(model=current_model_technical_name, messages=messages, temperature=0.7)\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        traceback.print_exc(); return f\"Kimi error: {e}\"\n",
    "\n",
    "def openai_chat_responder(user_message, chat_history):\n",
    "    try:\n",
    "        if current_model_technical_name == \"dall-e-3\":\n",
    "            result = openai_client.images.generate(\n",
    "                model=\"dall-e-3\",\n",
    "                prompt=user_message,\n",
    "                size=\"1024x1024\"\n",
    "            )\n",
    "            return f\"[Click to view image]({result.data[0].url})\"\n",
    "        else:\n",
    "            messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "            for user_msg, ai_msg in chat_history:\n",
    "                messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "                messages.append({\"role\": \"assistant\", \"content\": ai_msg})\n",
    "            messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "            response = openai_client.chat.completions.create(\n",
    "                model=current_model_technical_name,\n",
    "                messages=messages,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        traceback.print_exc(); return f\"OpenAI error: {e}\"\n",
    "\n",
    "def master_chat_responder(user_message, chat_history):\n",
    "    global current_model_technical_name\n",
    "    if not user_message.strip():\n",
    "        return \"Please type a message.\"\n",
    "    if 'moonshot' in current_model_technical_name:\n",
    "        if not kimi_configured:\n",
    "            return \"Error: Kimi API key not configured.\"\n",
    "        return kimi_chat_responder(user_message, chat_history)\n",
    "    elif 'gemini' in current_model_technical_name:\n",
    "        if not gemini_configured:\n",
    "            return \"Error: Gemini API key not configured.\"\n",
    "        return gemini_chat_responder(user_message, chat_history)\n",
    "    elif current_model_technical_name in [\"gpt-4.1\", \"gpt-4o-mini\", \"davinci-002\", \"dall-e-3\"]:\n",
    "        if not openai_configured:\n",
    "            return \"Error: OpenAI API key not configured.\"\n",
    "        return openai_chat_responder(user_message, chat_history)\n",
    "    else:\n",
    "        return \"Error: Unknown model type.\"\n",
    "\n",
    "# --- UI ---\n",
    "with gr.Blocks(theme=\"soft\", title=\"Multi-LLM Chat\") as demo:\n",
    "    gr.Markdown(\"# Multi-LLM Chat Dashboard (Kimi, Gemini & OpenAI)\")\n",
    "\n",
    "    load_and_configure()\n",
    "    initialize_api_clients()\n",
    "    current_model_technical_name = AVAILABLE_MODELS.get(DEFAULT_MODEL_DISPLAY_NAME)\n",
    "\n",
    "    with gr.Row():\n",
    "        model_dropdown = gr.Dropdown(\n",
    "            choices=list(AVAILABLE_MODELS.keys()),\n",
    "            value=DEFAULT_MODEL_DISPLAY_NAME,\n",
    "            label=\"Select LLM Model\"\n",
    "        )\n",
    "        model_status_text = gr.Textbox(\n",
    "            label=\"Model Status\",\n",
    "            value=f\"Current: {current_model_technical_name}\",\n",
    "            interactive=False\n",
    "        )\n",
    "\n",
    "    chat_interface = gr.ChatInterface(\n",
    "        fn=master_chat_responder,\n",
    "        chatbot=gr.Chatbot(height=500),\n",
    "        retry_btn=\"Retry\",\n",
    "        undo_btn=\"Undo\",\n",
    "        clear_btn=\"Clear\"\n",
    "    )\n",
    "\n",
    "    def handle_model_change(selected_display_name):\n",
    "        global current_model_technical_name, gemini_model\n",
    "        technical_name = AVAILABLE_MODELS.get(selected_display_name)\n",
    "        if 'moonshot' in technical_name and not kimi_configured:\n",
    "            return \"Cannot switch: Kimi API not configured.\"\n",
    "        if 'gemini' in technical_name and not gemini_configured:\n",
    "            return \"Cannot switch: Gemini API not configured.\"\n",
    "        if technical_name in [\"gpt-4.1\", \"gpt-4o-mini\", \"davinci-002\", \"dall-e-3\"] and not openai_configured:\n",
    "            return \"Cannot switch: OpenAI API not configured.\"\n",
    "        if current_model_technical_name and 'gemini' in current_model_technical_name:\n",
    "            gemini_model = None\n",
    "        current_model_technical_name = technical_name\n",
    "        return f\"Switched to model: {current_model_technical_name}\"\n",
    "\n",
    "    model_dropdown.change(fn=handle_model_change, inputs=[model_dropdown], outputs=[model_status_text])\n",
    "\n",
    "    if not gemini_configured:\n",
    "        gr.Markdown(\"<p style='color:orange;'>Warning: Gemini API not configured.</p>\")\n",
    "    if not kimi_configured:\n",
    "        gr.Markdown(\"<p style='color:orange;'>Warning: Kimi API not configured.</p>\")\n",
    "    if not openai_configured:\n",
    "        gr.Markdown(\"<p style='color:orange;'>Warning: OpenAI API not configured.</p>\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(server_name=\"0.0.0.0\", share=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d264a-68cc-4704-a069-f3ef08e8d3b4",
   "metadata": {},
   "source": [
    "not working\n",
    "Since 10 July 2025 the **official** Moonshot endpoint exposes the following **built-in functions** (all prefixed with `$`):\n",
    "\n",
    "| Function name | Purpose | Extra cost |\n",
    "|---------------|---------|------------|\n",
    "| `$web_search` | Real-time web search & summarisation | ¥0.03 per invocation |\n",
    "| `$image_gen`  | On-the-fly image generation via DALL-E 3 | ¥0.08 per 512×512 image |\n",
    "| `$code_runner`| Sand-boxed Python code execution (max 30 s, 256 MB RAM) | ¥0.01 per run |\n",
    "| `$file_parser`| OCR / extract text from uploaded PDF, DOCX, PPTX, XLSX, etc. | ¥0.005 per page / slide |\n",
    "| `$speech_rec` | Whisper-based speech-to-text for uploaded audio (mp3/wav/m4a) | ¥0.006 per 10 s |\n",
    "\n",
    "Usage pattern is identical to `$web_search`; just change the `\"name\"` field.\n",
    "\n",
    "Example (run Python code):\n",
    "\n",
    "```python\n",
    "tools=[{\n",
    "    \"type\": \"builtin_function\",\n",
    "    \"function\": {\"name\": \"$code_runner\"}\n",
    "}]\n",
    "```\n",
    "\n",
    "Everything else (parameters, billing, token accounting) is handled transparently by Moonshot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allensdk",
   "language": "python",
   "name": "allensdk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
