{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25710525-72a3-4362-8544-7337bf771ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from openai import OpenAI as OpenAIClient\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import urllib3\n",
    "import requests\n",
    "import sqlite3\n",
    "import shutil\n",
    "import json # For simple settings\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "# --- Path Setup for 'assets' ---\n",
    "project_root_path = Path(os.path.abspath(os.getcwd()))\n",
    "assets_dir = project_root_path / 'assets'\n",
    "if str(assets_dir) not in sys.path and assets_dir.exists():\n",
    "    sys.path.append(str(assets_dir))\n",
    "    print(f\"Added to sys.path: {assets_dir}\")\n",
    "\n",
    "# --- Import from assets ---\n",
    "try:\n",
    "    from func_inputoutput import manage_conversation_history, word_count\n",
    "    # Attempt to import DocumentRetriever if it's there\n",
    "    try:\n",
    "        from func_inputoutput import DocumentRetriever as AssetDocumentRetriever\n",
    "        print(\"Using DocumentRetriever from assets.func_inputoutput.py\")\n",
    "    except ImportError:\n",
    "        AssetDocumentRetriever = None\n",
    "        print(\"DocumentRetriever not found in assets.func_inputoutput.py, will use inline definition.\")\n",
    "    # Try to import settings functions\n",
    "    try:\n",
    "        from func_inputoutput import save_settings as fio_save_settings\n",
    "        from func_inputoutput import load_settings as fio_load_settings # This now refers to the simplified version\n",
    "        print(\"Using save_settings/load_settings from func_inputoutput.py\")\n",
    "\n",
    "        def save_ingestion_settings(settings_data, filename=\"pdf_ingestion_settings.json\"):\n",
    "            return fio_save_settings(settings_data, filename=filename) # This call is fine\n",
    "\n",
    "        def load_ingestion_settings(filename=\"pdf_ingestion_settings.json\"):\n",
    "            # fio_load_settings now directly returns a dict or {}\n",
    "            loaded_dict = fio_load_settings(filename=filename) # No need for return_dict_directly anymore\n",
    "            \n",
    "            # This check is still good as a safeguard, though fio_load_settings should always return a dict.\n",
    "            if not isinstance(loaded_dict, dict):\n",
    "                print(f\"Warning: fio_load_settings from '{filename}' did not return a dict as expected. Using empty settings.\")\n",
    "                return {} # Fallback\n",
    "            \n",
    "            default_ui_settings = {\n",
    "                # 'ingest_input_path': '', # Commented out as gr.File doesn't use default string path\n",
    "                'ingest_output_dir': str(project_root_path / \"processed_databases\"),\n",
    "                'ingest_db_name_stem': 'processed_docs',\n",
    "                'ingest_processing_mode': \"grobid\",\n",
    "                'ingest_overwrite_db': False,\n",
    "                'ingest_grobid_config': 'config.json',\n",
    "                'ingest_server_directory_path': '' # Ensure this default is present\n",
    "            }\n",
    "            final_settings = {**default_ui_settings, **loaded_dict}\n",
    "            print(f\"Loaded UI settings dict from '{filename}': {final_settings}\")\n",
    "            return final_settings\n",
    "        \n",
    "        \n",
    "    except ImportError as e:        \n",
    "        print(\"Could not load Settings\")\n",
    "except ImportError as e:\n",
    "    print(f\"ERROR: Could not import core functions from func_inputoutput.py in {assets_dir}: {e}\")\n",
    "    # Define basic settings functions if import fails for some reason and they were expected\n",
    "    if 'fio_save_settings' not in globals():\n",
    "        def save_ingestion_settings(settings_data, filename=\"pdf_ingestion_settings.json\"): print(\"Error: save_settings N/A\")\n",
    "        def load_ingestion_settings(filename=\"pdf_ingestion_settings.json\"): return {}\n",
    "    sys.exit(1) # Or handle more gracefully\n",
    "\n",
    "# --- Import PDF Processor ---\n",
    "try:\n",
    "    from pdftosqlite_processor import process_documents_to_sqlite\n",
    "    print(\"pdftosqlite_processor.py loaded successfully.\")\n",
    "except ImportError as e:\n",
    "    print(f\"ERROR: Could not import from pdftosqlite_processor.py: {e}\")\n",
    "    print(\"Please ensure pdftosqlite_processor.py is in the same directory or Python path.\")\n",
    "    # Define a dummy function so the UI doesn't crash on launch\n",
    "    def process_documents_to_sqlite(*args, **kwargs):\n",
    "        return \"Error: pdftosqlite_processor.py not found or failed to import.\", None\n",
    "    # sys.exit(1) # Or allow app to run with this feature disabled\n",
    "\n",
    "# --- Proxy Setup ---\n",
    "os.environ['NO_PROXY'] = 'localhost,127.0.0.1,127.0.0.1:8070' # Added Grobid port\n",
    "urllib3.disable_warnings()\n",
    "if hasattr(urllib3.util.connection, 'is_connection_dropped'): # type: ignore\n",
    "    urllib3.util.connection.is_connection_dropped = lambda conn: False # type: ignore\n",
    "if hasattr(requests.Session(), 'trust_env'):\n",
    "    requests.Session().trust_env = False # type: ignore\n",
    "print(f\"Using NO_PROXY: {os.environ.get('NO_PROXY')}\")\n",
    "\n",
    "# --- Initialize OpenAI Client ---\n",
    "try:\n",
    "    oai_client = OpenAIClient(base_url=\"http://localhost:1238/v1\", api_key=\"lm-studio\")\n",
    "    print(\"OpenAI client initialized successfully for LM Studio.\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR initializing OpenAI client: {e}. Ensure LM Studio is running on http://localhost:1238/v1.\")\n",
    "    oai_client = None # type: ignore\n",
    "\n",
    "\n",
    "BASE_DOCS_PATH = project_root_path / \"docs\"\n",
    "INGESTION_SETTINGS_FILE = \"pdf_ingestion_settings.json\" # For the new tab\n",
    "# --- Inline DocumentRetriever Definition (if not imported from assets) ---\n",
    "if AssetDocumentRetriever is None:\n",
    "    print(\"Defining DocumentRetriever inline.\")\n",
    "    class DocumentRetriever:\n",
    "        def __init__(self, vectordb: Chroma, openai_client: Optional[OpenAIClient] = oai_client):\n",
    "            self.vectordb = vectordb\n",
    "            self.openai_client = openai_client\n",
    "\n",
    "        def retrieve_documents(self, query: str, is_first_run: bool, k: int = 10, method: str = 'combined') -> Tuple[str, str, str]:\n",
    "            retrieved_text = \"\"\n",
    "            refined_query_for_display = query \n",
    "            similar_docs = [] # Initialize similar_docs\n",
    "\n",
    "            if '{do not use retrieval}' in query:\n",
    "                return \" \", \"Retrieval skipped as per '{do not use retrieval}' instruction.\", method\n",
    "\n",
    "            if query.lower().startswith(\"doc_id:\"):\n",
    "                try:\n",
    "                    doc_id_str = query.split(\":\", 1)[1].strip().split(\",\")[0]\n",
    "                    doc_id_val = int(doc_id_str)\n",
    "                    method = \"direct_doc_id_search\"\n",
    "                    similar_docs_content = self.search_vectordb_by_id_chat('ID', doc_id_val, k)\n",
    "                    \n",
    "                    if similar_docs_content and similar_docs_content != \"No documents found.\":\n",
    "                        refined_query_for_display = f\"Found documents for ID: {doc_id_val}\"\n",
    "                    else:\n",
    "                        refined_query_for_display = f\"No documents found for ID: {doc_id_val}\"\n",
    "                    return similar_docs_content, refined_query_for_display, method\n",
    "                except ValueError:\n",
    "                    return \"Error: Invalid Doc ID format. Must be an integer after 'doc_id:'.\", \"Invalid Doc ID\", method\n",
    "                except Exception as e:\n",
    "                    return f\"Error during doc_id search: {e}\", \"Doc ID Search Error\", method\n",
    "\n",
    "            if is_first_run:\n",
    "                refined_query_for_retrieval = \"\" \n",
    "                refined_query_for_display = \"Initial greeting, no retrieval performed.\"\n",
    "            else:\n",
    "                actual_query_for_llm_refinement = query.replace('{no history}', '').strip()\n",
    "                if method == 'keywords':\n",
    "                    refined_query_for_retrieval = self.extract_keywords(actual_query_for_llm_refinement)\n",
    "                elif method == 'llm':\n",
    "                    refined_query_for_retrieval = self.generate_useful_query(actual_query_for_llm_refinement)\n",
    "                elif method == 'combined':\n",
    "                    keywords = self.extract_keywords(actual_query_for_llm_refinement)\n",
    "                    llm_refined = self.generate_useful_query(actual_query_for_llm_refinement)\n",
    "                    refined_query_for_retrieval = f\"{llm_refined} {keywords}\".strip() \n",
    "                else: \n",
    "                    refined_query_for_retrieval = actual_query_for_llm_refinement\n",
    "                \n",
    "                refined_query_for_display = refined_query_for_retrieval \n",
    "\n",
    "                if self.is_query_meaningful(refined_query_for_retrieval):\n",
    "                    try:\n",
    "                        similar_docs = self.vectordb.similarity_search(refined_query_for_retrieval, k=k)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error during similarity search: {e}\")\n",
    "                        return f\"Error during similarity search: {e}\", refined_query_for_display, method\n",
    "                else:\n",
    "                    refined_query_for_display = \"No meaningful query generated. No retrieval performed.\"\n",
    "                    return \" \", refined_query_for_display, method\n",
    "            \n",
    "            if similar_docs:\n",
    "                for i, doc in enumerate(similar_docs):\n",
    "                    doc_id = doc.metadata.get('doc_id', 'N/A') \n",
    "                    chunk_id = doc.metadata.get('chunk_id', 'N/A')\n",
    "                    retrieved_text += f\"**Document {doc_id}, Chunk {chunk_id}**:\\n{doc.page_content}\\n\\n\"\n",
    "            else:\n",
    "                if not is_first_run : \n",
    "                     return \"No relevant documents found for your query.\", refined_query_for_display, method\n",
    "\n",
    "            return retrieved_text.strip(), refined_query_for_display, method\n",
    "\n",
    "        def is_query_meaningful(self, query: str) -> bool:\n",
    "            if not query or len(query.strip()) < 3: return False\n",
    "            query_lower = query.lower()\n",
    "            meaningless_phrases = [\n",
    "                'no content found', 'no keywords', 'not specified', 'empty query',\n",
    "                'no llm', 'no retrieval', 'refined query', 'test', 'tests', 'search for'\n",
    "            ]\n",
    "            if any(phrase in query_lower for phrase in meaningless_phrases): return False\n",
    "            if query_lower == query.lower().strip() and len(query.split()) < 2 and len(query) < 5 : \n",
    "                if query_lower not in [\"hello\", \"hi\"]: \n",
    "                    pass\n",
    "            return True\n",
    "\n",
    "        def extract_keywords(self, query: str) -> str:\n",
    "            keywords = re.findall(r'\\{(.*?)\\}', query)\n",
    "            return ', '.join(keywords) if keywords else ''\n",
    "\n",
    "        def generate_useful_query(self, query: str) -> str:\n",
    "            if not self.openai_client:\n",
    "                print(\"Warning: OpenAI client not available for generate_useful_query. Returning original query.\")\n",
    "                return query\n",
    "            instruction = (\"Extract named entities, specific technical terms, and key concepts \"\n",
    "                           \"from the following query that are most relevant for a semantic vector search. \"\n",
    "                           \"Provide ONLY these entities/terms/concepts, separated by spaces or commas. \"\n",
    "                           \"Focus on proper nouns, specific technologies, or multi-word key phrases. \"\n",
    "                           \"Do not include conversational filler or instructions like 'search for'.\")\n",
    "            prompt_template_str = \"{instruction}\\nOriginal Query: \\\"{query}\\\"\\nRefined Search Terms:\"\n",
    "            prompt = prompt_template_str.format(query=query, instruction=instruction)\n",
    "            try:\n",
    "                completion = self.openai_client.chat.completions.create(\n",
    "                    model=\"lmstudio/Meta-Llama-3.1\", \n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are an expert at extracting precise semantic search terms from user queries.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],              \n",
    "                    temperature=0.2, stream=False,\n",
    "                )\n",
    "                full_response = completion.choices[0].message.content.strip()\n",
    "                return full_response if full_response else query \n",
    "            except Exception as e:\n",
    "                print(f\"Error in LLM query generation: {e}\")\n",
    "                return query\n",
    "\n",
    "        def search_vectordb_by_id_chat(self, search_field: str, search_value: Any, k: int = 3) -> str:\n",
    "            if not self.vectordb or not hasattr(self.vectordb, '_collection'):\n",
    "                return \"Error: VectorDB not properly initialized for ID search.\"\n",
    "            collection = self.vectordb._collection\n",
    "            try:\n",
    "                results = collection.get(where={\"doc_id\": search_value}) \n",
    "                if results and results['documents']:\n",
    "                    doc_meta_pairs = []\n",
    "                    for i in range(len(results['ids'])): \n",
    "                        if i < len(results['metadatas']) and results['metadatas'][i] is not None:\n",
    "                            doc_meta_pairs.append({\n",
    "                                \"content\": results['documents'][i],\n",
    "                                \"metadata\": results['metadatas'][i],\n",
    "                                \"id\": results['ids'][i] \n",
    "                            })\n",
    "                        else: \n",
    "                             doc_meta_pairs.append({\n",
    "                                \"content\": results['documents'][i],\n",
    "                                \"metadata\": {\"doc_id\": search_value, \"chunk_id\": \"unknown\"}, \n",
    "                                \"id\": results['ids'][i]\n",
    "                            })\n",
    "                    sorted_chunks = sorted(\n",
    "                        doc_meta_pairs,\n",
    "                        key=lambda x: x[\"metadata\"].get('chunk_id', float('inf')) \n",
    "                    )\n",
    "                    formatted_texts = []\n",
    "                    for item in sorted_chunks:\n",
    "                        chunk_id = item[\"metadata\"].get('chunk_id', 'N/A')\n",
    "                        title = item[\"metadata\"].get('Title', 'N/A')\n",
    "                        doc_text = (\n",
    "                            f\"**Document ID {search_value} (Title: {title}), Chunk {chunk_id}**:\\n\"\n",
    "                            f\"{item['content']}\\n\"\n",
    "                        )\n",
    "                        formatted_texts.append(doc_text.strip())\n",
    "                    return \"\\n\\n\".join(formatted_texts) if formatted_texts else \"No content found for this Document ID after filtering/sorting.\"\n",
    "                else:\n",
    "                    return \"No documents found for this Document ID.\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error searching vectordb by ID: {e}\")\n",
    "                return f\"Error occurred during Document ID search: {str(e)}\"\n",
    "    DocumentRetrieverClass = DocumentRetriever\n",
    "else:\n",
    "    DocumentRetrieverClass = AssetDocumentRetriever # type: ignore\n",
    "    print(\"Using DocumentRetriever from assets file.\")\n",
    "\n",
    "# --- Embedding Class ---\n",
    "class CustomEmbeddingForGradio:\n",
    "    def __init__(self, openai_client: Optional[OpenAIClient]):\n",
    "        self.client = openai_client\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        if not self.client: raise ValueError(\"OpenAI client not initialized for embeddings.\")\n",
    "        return [self.get_embedding(text) for text in texts]\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        if not self.client: raise ValueError(\"OpenAI client not initialized for embeddings.\")\n",
    "        return self.get_embedding(text)\n",
    "    def get_embedding(self, text: str, model: str = \"nomic-embed-text\") -> List[float]:\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        if not self.client: raise ConnectionError(\"OpenAI client is not available for embeddings.\")\n",
    "        try:\n",
    "            response = self.client.embeddings.create(input=[text], model=model)\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR during embedding generation for text '{text[:50]}...': {e}\")\n",
    "            raise\n",
    "embedding_function = CustomEmbeddingForGradio(openai_client=oai_client)\n",
    "\n",
    "# --- Database Processing Functions (for RAG ChromaDB) ---\n",
    "def load_docs_from_sqlite2(sqlite_db_path: str, table_name: str = \"document_table\", # Defaulted to new table name\n",
    "                          id_column: str = \"ID\", text_column: str = \"Abstract\", \n",
    "                          body_column: str = \"Body\", title_column: str = \"Title\") \\\n",
    "                          -> Tuple[List[Document], str, int]:\n",
    "    documents = []\n",
    "    processed_ids = set()  # Track unique document IDs\n",
    "    num_original_docs = 0\n",
    "    try:\n",
    "        conn = sqlite3.connect(sqlite_db_path)\n",
    "        cursor = conn.cursor()\n",
    "        # First, let's count total records\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        total_records = cursor.fetchone()[0]\n",
    "        print(f\"Total records in databasetb: {total_records}\")\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name=?;\", (table_name,))\n",
    "        if not cursor.fetchone():\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "            tables = cursor.fetchall()\n",
    "            if not tables:\n",
    "                conn.close()\n",
    "                return [], f\"No tables found in SQLite DB: {sqlite_db_path}\", 0\n",
    "            table_name = tables[0][0] # Use first table if 'document_table' not found\n",
    "            print(f\"Table '{table_name}' not found, using first available table: '{table_name}'\")\n",
    "\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "        available_columns = [row[1].lower() for row in cursor.fetchall()]\n",
    "        \n",
    "        select_cols_map = { \"id\": id_column, \"title\": title_column, \"abstract\": text_column, \"body\": body_column }\n",
    "        actual_select_cols = []\n",
    "        final_col_names = []\n",
    "\n",
    "        for key, preferred_name in select_cols_map.items():\n",
    "            if preferred_name.lower() in available_columns:\n",
    "                actual_select_cols.append(preferred_name)\n",
    "                final_col_names.append(key)\n",
    "            else:\n",
    "                print(f\"Warning: Column '{preferred_name}' for '{key}' not found in table '{table_name}'.\")\n",
    "        \n",
    "        if not actual_select_cols or (\"abstract\" not in final_col_names and \"body\" not in final_col_names):\n",
    "            conn.close()\n",
    "            return [], f\"Neither abstract nor body columns (or any specified content columns) found. Cannot process.\", 0\n",
    "        if \"id\" not in final_col_names:\n",
    "            conn.close()\n",
    "            return [], f\"ID column '{id_column}' not found. Cannot process.\", 0\n",
    "\n",
    "        query = f\"SELECT {', '.join(actual_select_cols)} FROM {table_name}\"\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "        conn.close()\n",
    "\n",
    "        for row_data in results:\n",
    "            row = dict(zip(final_col_names, row_data))\n",
    "            record_id = row.get(\"id\")\n",
    "            title_content = row.get(\"title\", f\"Untitled Doc {record_id}\")\n",
    "            abstract_content = str(row.get(\"abstract\", \"\"))\n",
    "            body_content = str(row.get(\"body\", \"\"))\n",
    "            \n",
    "            # Fix the variable names here\n",
    "            if abstract_content is None:\n",
    "                abstract_content = \"\"\n",
    "            if body_content is None:\n",
    "                body_content = \"\"\n",
    "            \n",
    "            combined_text = (abstract_content + \" \" + body_content).strip()\n",
    "\n",
    "            if combined_text:\n",
    "                processed_ids.add(record_id)  # Track this ID\n",
    "                num_original_docs += 1\n",
    "                metadata = {\n",
    "                    'ID': record_id if record_id is not None else \"\",\n",
    "                    'Title': title_content if title_content is not None else \"\",  # Also fixed title variable\n",
    "                }\n",
    "                doc = Document(page_content=combined_text, metadata=metadata)\n",
    "                documents.append(doc)\n",
    "        \n",
    "        if not documents:\n",
    "            return [], f\"No processable documents found in {sqlite_db_path} (table: {table_name}).\", 0\n",
    "        \n",
    "        return documents, f\"Loaded {len(processed_ids)} unique documents from {sqlite_db_path} (table: {table_name}).\", len(processed_ids)\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        return [], f\"SQLite error processing {sqlite_db_path}: {e}\", 0\n",
    "    except Exception as e:\n",
    "        return [], f\"Unexpected error processing {sqlite_db_path}: {e}\", 0\n",
    "\n",
    "\n",
    "\n",
    "def load_docs_from_sqlite(\n",
    "    sqlite_db_path: str,\n",
    "    table_name: str = \"document_table\",\n",
    "    id_column: str = \"ID\",  # Column for the primary ID of the document in metadata['ID']\n",
    "    title_column: str = \"Title\",\n",
    "    abstract_column: str = \"Abstract\",\n",
    "    body_column: str = \"Body\",\n",
    "    # For EndNote Record Number or other specific IDs you want in metadata:\n",
    "    # Maps desired metadata key to actual DB column name\n",
    "    additional_metadata_cols: Dict[str, str] = None\n",
    ") -> Tuple[List[Document], str, int]:\n",
    "    \"\"\"\n",
    "    Loads documents from an SQLite database, creating Langchain Document objects.\n",
    "\n",
    "    Args:\n",
    "        sqlite_db_path: Path to the SQLite database file.\n",
    "        table_name: Name of the table containing the documents.\n",
    "        id_column: Name of the DB column to use for metadata['ID']. This ID is used\n",
    "                   to count unique documents.\n",
    "        title_column: Name of the DB column for the title.\n",
    "        abstract_column: Name of the DB column for the abstract.\n",
    "        body_column: Name of the DB column for the body text.\n",
    "        additional_metadata_cols: Dictionary mapping desired metadata keys to actual\n",
    "                                   DB column names for other fields to include.\n",
    "                                   Example: {\"endnote_id\": \"RecordNumberFromEndnote\"}\n",
    "\n",
    "    Returns:\n",
    "        A tuple: (list of Documents, status message, count of unique documents loaded).\n",
    "    \"\"\"\n",
    "    documents: List[Document] = []\n",
    "    processed_sqlite_ids = set() # Tracks unique IDs from the id_column\n",
    "    conn = None\n",
    "\n",
    "    try:\n",
    "        conn = sqlite3.connect(sqlite_db_path)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # 1. Determine actual table name (fallback if specified one not found)\n",
    "        actual_table_name = table_name\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name=?;\", (actual_table_name,))\n",
    "        if not cursor.fetchone():\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "            tables = cursor.fetchall()\n",
    "            if not tables:\n",
    "                return [], f\"No tables found in SQLite DB: {sqlite_db_path}\", 0\n",
    "            actual_table_name = tables[0][0]\n",
    "            print(f\"Warning: Table '{table_name}' not found. Using first available table: '{actual_table_name}'\")\n",
    "\n",
    "        # 2. CRITICAL DIAGNOSTIC: Count total records in the identified table\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {actual_table_name}\")\n",
    "        total_records_in_db_table = cursor.fetchone()[0]\n",
    "        print(f\"Total records found in SQLite table '{actual_table_name}': {total_records_in_db_table}\")\n",
    "\n",
    "        # 3. Get available columns from the table (lowercased for matching, store original case)\n",
    "        cursor.execute(f\"PRAGMA table_info({actual_table_name})\")\n",
    "        db_columns_info = {row[1].lower(): row[1] for row in cursor.fetchall()} # {lowercase_name: OriginalCaseName}\n",
    "\n",
    "        # 4. Define which columns to fetch and how to map them\n",
    "        # Core content/ID columns\n",
    "        # Key: standardized key used in code; Value: preferred DB column name from function args\n",
    "        column_selection_map = {\n",
    "            \"doc_id_val\": id_column,       # For metadata['ID'] and uniqueness check\n",
    "            \"title_val\": title_column,\n",
    "            \"abstract_val\": abstract_column,\n",
    "            \"body_val\": body_column,\n",
    "        }\n",
    "        # Add any additional metadata columns specified by the user\n",
    "        if additional_metadata_cols:\n",
    "            for meta_key, db_col_name in additional_metadata_cols.items():\n",
    "                if meta_key not in column_selection_map: # Avoid overwriting core keys\n",
    "                    column_selection_map[meta_key] = db_col_name\n",
    "                else:\n",
    "                    print(f\"Warning: Additional metadata key '{meta_key}' conflicts with a core processing key. Ignoring.\")\n",
    "\n",
    "        select_query_parts = []  # DB column names for SELECT statement\n",
    "        active_processing_keys = [] # Standardized keys corresponding to select_query_parts\n",
    "\n",
    "        for key, preferred_db_col_name in column_selection_map.items():\n",
    "            if preferred_db_col_name.lower() in db_columns_info:\n",
    "                select_query_parts.append(db_columns_info[preferred_db_col_name.lower()]) # Use original DB column case\n",
    "                active_processing_keys.append(key)\n",
    "            else:\n",
    "                # Warn only if essential columns are missing\n",
    "                if key in [\"doc_id_val\", \"abstract_val\", \"body_val\"]: # title is optional-ish\n",
    "                    print(f\"Warning: Column '{preferred_db_col_name}' (for internal key '{key}') not found in table '{actual_table_name}'.\")\n",
    "\n",
    "        if \"doc_id_val\" not in active_processing_keys:\n",
    "            return [], f\"Required ID column '{id_column}' (for metadata['ID']) not found in table '{actual_table_name}'. Cannot process.\", 0\n",
    "        if \"abstract_val\" not in active_processing_keys and \"body_val\" not in active_processing_keys:\n",
    "            return [], f\"Neither abstract column ('{abstract_column}') nor body column ('{body_column}') found. No content to process.\", 0\n",
    "        if not select_query_parts:\n",
    "             return [], f\"No columns to select based on specified parameters from table '{actual_table_name}'.\", 0\n",
    "\n",
    "\n",
    "        # 5. Execute query\n",
    "        query = f\"SELECT {', '.join(select_query_parts)} FROM {actual_table_name}\"\n",
    "        # print(f\"DEBUG: Executing query: {query}\")\n",
    "        cursor.execute(query)\n",
    "        \n",
    "        # 6. Process fetched rows\n",
    "        fetched_rows = cursor.fetchall()\n",
    "        # print(f\"DEBUG: Number of rows fetched by query: {len(fetched_rows)}\")\n",
    "\n",
    "        for row_tuple in fetched_rows:\n",
    "            row_dict = dict(zip(active_processing_keys, row_tuple))\n",
    "\n",
    "            # Primary ID for the document object (will go into metadata['ID'])\n",
    "            current_doc_id = row_dict.get(\"doc_id_val\")\n",
    "\n",
    "            title_content = str(row_dict.get(\"title_val\", \"\")).strip()\n",
    "            if not title_content and current_doc_id is not None:\n",
    "                title_content = f\"Untitled Document {current_doc_id}\"\n",
    "            elif not title_content:\n",
    "                title_content = \"Untitled Document\"\n",
    "\n",
    "            abstract_content = str(row_dict.get(\"abstract_val\", \"\")).strip()\n",
    "            body_content = str(row_dict.get(\"body_val\", \"\")).strip()\n",
    "            \n",
    "            combined_text = (abstract_content + \" \" + body_content).strip()\n",
    "\n",
    "            if combined_text:  # Only create a Document if there's actual text content\n",
    "                if current_doc_id is not None: # Ensure ID is not None before adding to set\n",
    "                    processed_sqlite_ids.add(current_doc_id)\n",
    "                \n",
    "                # Prepare metadata\n",
    "                metadata = {\n",
    "                    'ID': current_doc_id, # This 'ID' is what chunk_texts_with_metadata expects\n",
    "                    'Title': title_content,\n",
    "                    'source_db_table': actual_table_name # Example of useful extra metadata\n",
    "                }\n",
    "\n",
    "                # Add other fetched additional_metadata_cols\n",
    "                for key, db_col_name in (additional_metadata_cols or {}).items():\n",
    "                    if key in row_dict: # If it was successfully fetched\n",
    "                        metadata[key] = row_dict[key]\n",
    "                \n",
    "                doc = Document(page_content=combined_text, metadata=metadata)\n",
    "                documents.append(doc)\n",
    "        \n",
    "        num_unique_docs_loaded = len(processed_sqlite_ids)\n",
    "        \n",
    "        if not documents:\n",
    "            msg = (f\"No processable documents (with content) found in table '{actual_table_name}'. \"\n",
    "                   f\"Total records in DB table was: {total_records_in_db_table}. \"\n",
    "                   f\"Rows fetched by query: {len(fetched_rows)}.\")\n",
    "            return [], msg, 0\n",
    "        \n",
    "        status_msg = (f\"Loaded {num_unique_docs_loaded} unique documents (based on '{id_column}' values) \"\n",
    "                      f\"from table '{actual_table_name}'. \"\n",
    "                      f\"Total Langchain Documents created: {len(documents)}. \"\n",
    "                      f\"(SQLite table initially had {total_records_in_db_table} records).\")\n",
    "        return documents, status_msg, num_unique_docs_loaded\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        return [], f\"SQLite error processing '{sqlite_db_path}' (table: '{table_name}'): {e}\", 0\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        # print(f\"Unexpected error details: {traceback.format_exc()}\") # Uncomment for detailed debug\n",
    "        return [], f\"Unexpected error processing '{sqlite_db_path}' (table: '{table_name}'): {e}\", 0\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()    \n",
    "    \n",
    "\n",
    "def chunk_texts_with_metadata2(docs: List[Document], chunk_size: int = 2000, chunk_overlap: int = 200) -> List[Document]:\n",
    "    # ... (your existing chunk_texts_with_metadata function - no changes needed here)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunked_texts = text_splitter.split_documents(docs)\n",
    "    # Track which original documents got chunked\n",
    "    chunked_doc_ids = set()\n",
    "    \n",
    "    for i, text_chunk in enumerate(chunked_texts):\n",
    "        original_id = text_chunk.metadata.get('ID')\n",
    "        if original_id: chunked_doc_ids.add(original_id)\n",
    "        \n",
    "        text_chunk.metadata['doc_id'] = text_chunk.metadata.get('ID', f'doc_unknown_{i}')\n",
    "        text_chunk.metadata['chunk_id'] = i\n",
    "    \n",
    "    #print(f\"Chunks represent {len(chunked_doc_ids)} unique original documents\")\n",
    "    return chunked_texts\n",
    "\n",
    "\n",
    "def chunk_texts_with_metadata(\n",
    "    docs: List[Document], \n",
    "    chunk_size: int = 2000, \n",
    "    chunk_overlap: int = 200,\n",
    "    original_id_key: str = 'ID' # The metadata key for the original document's ID\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Chunks documents and assigns 'doc_id' and 'chunk_id' metadata.\n",
    "    'doc_id' will be the ID of the original document (from original_id_key or a fallback).\n",
    "    'chunk_id' will be a globally unique sequential ID for each chunk.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    all_processed_chunks: List[Document] = []\n",
    "    # This set will store the unique identifiers of the original documents processed.\n",
    "    # These identifiers will be what we assign as 'doc_id' to the chunks.\n",
    "    processed_original_doc_ids: Set[Any] = set() \n",
    "    \n",
    "    global_chunk_counter = 0\n",
    "\n",
    "    for original_doc_index, original_doc in enumerate(docs):\n",
    "        # Determine the identifier for this original document.\n",
    "        # Priority 1: Use the value from original_doc.metadata[original_id_key]\n",
    "        # Priority 2: Fallback to a generated ID based on the document's index in the input list.\n",
    "        \n",
    "        current_original_doc_id = None\n",
    "        if original_doc.metadata and original_id_key in original_doc.metadata:\n",
    "            current_original_doc_id = original_doc.metadata[original_id_key]\n",
    "        \n",
    "        if current_original_doc_id is None:\n",
    "            # If the ID is missing or None, create a fallback ID.\n",
    "            # This ensures that all chunks from *this specific original_doc object*\n",
    "            # will share the same 'doc_id', even if it's a generated one.\n",
    "            current_original_doc_id = f\"original_doc_idx_{original_doc_index}\"\n",
    "            # You might want to log a warning here if an ID was expected but not found.\n",
    "            # print(f\"Warning: Original document at index {original_doc_index} missing '{original_id_key}'. Using fallback ID: {current_original_doc_id}\")\n",
    "\n",
    "        processed_original_doc_ids.add(current_original_doc_id)\n",
    "        \n",
    "        # Split the current original document.\n",
    "        # Note: text_splitter.split_documents expects a list.\n",
    "        # Metadata from original_doc is automatically propagated to its chunks.\n",
    "        chunks_from_this_doc = text_splitter.split_documents([original_doc])\n",
    "        \n",
    "        for text_chunk in chunks_from_this_doc:\n",
    "            # Ensure metadata dictionary exists\n",
    "            if text_chunk.metadata is None:\n",
    "                text_chunk.metadata = {}\n",
    "            \n",
    "            # Assign 'doc_id': the identifier of the original document this chunk came from.\n",
    "            text_chunk.metadata['doc_id'] = current_original_doc_id\n",
    "            \n",
    "            # Assign 'chunk_id': a globally unique sequential ID for this specific chunk.\n",
    "            text_chunk.metadata['chunk_id'] = global_chunk_counter\n",
    "            \n",
    "            all_processed_chunks.append(text_chunk)\n",
    "            global_chunk_counter += 1\n",
    "            \n",
    "    # This print statement now accurately reflects the number of unique original document identifiers\n",
    "    # that were processed and assigned as 'doc_id' to chunks.\n",
    "    print(f\"Generated {len(all_processed_chunks)} chunks.\")\n",
    "    print(f\"These chunks represent {len(processed_original_doc_ids)} unique original documents (based on '{original_id_key}' or fallback index).\")\n",
    "    \n",
    "    return all_processed_chunks\n",
    "\n",
    "def create_or_load_chromadb(texts_to_add: Optional[List[Document]], embedding_fn: Any, \n",
    "                            persist_dir: str, mode: str = \"create\", force_overwrite: bool = True) \\\n",
    "                            -> Tuple[Optional[Chroma], str, int]:\n",
    "    status_message = \"\"\n",
    "    db = None\n",
    "    num_chunks = 0\n",
    "    persist_path = Path(persist_dir)\n",
    "\n",
    "    if mode == \"create\":\n",
    "        if persist_path.exists():\n",
    "            if list(persist_path.iterdir()): # Check if directory is not empty\n",
    "                if force_overwrite:\n",
    "                    shutil.rmtree(persist_path)\n",
    "                    status_message += f\"Overwriting existing DB at {persist_path}. \"\n",
    "                    persist_path.mkdir(parents=True, exist_ok=True)\n",
    "                else:\n",
    "                    return None, f\"DB directory '{persist_path}' is not empty. Use 'Load Existing' or enable overwrite.\", 0\n",
    "            else: \n",
    "                try: persist_path.rmdir() # Try removing empty dir first\n",
    "                except OSError: shutil.rmtree(persist_path) # If hidden files, use rmtree\n",
    "                persist_path.mkdir(parents=True, exist_ok=True)\n",
    "        else:\n",
    "            persist_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        if not texts_to_add:\n",
    "             return None, \"No texts provided to create new ChromaDB.\", 0\n",
    "        try:\n",
    "            print(f\"Attempting to create ChromaDB with {len(texts_to_add)} text chunks in {persist_path}...\")\n",
    "            start_time = time.time()\n",
    "            db = Chroma.from_documents(texts_to_add, embedding_fn, persist_directory=str(persist_path))\n",
    "            end_time = time.time()\n",
    "            num_chunks = db._collection.count() if db and hasattr(db, '_collection') else 0\n",
    "            status_message += f\"New ChromaDB created in {end_time - start_time:.2f}s at {persist_path}. Chunks: {num_chunks}.\"\n",
    "            print(status_message)\n",
    "        except Exception as e:\n",
    "            err_msg = f\"Error creating ChromaDB: {e}\"\n",
    "            print(err_msg)\n",
    "            return None, err_msg, 0\n",
    "\n",
    "    elif mode == \"load\":\n",
    "        if not persist_path.exists() or not (persist_path / \"chroma.sqlite3\").exists():\n",
    "            return None, f\"ChromaDB not found at {persist_path} (or missing chroma.sqlite3). Cannot load.\", 0\n",
    "        try:\n",
    "            print(f\"Attempting to load ChromaDB from {persist_path}...\")\n",
    "            start_time = time.time()\n",
    "            db = Chroma(persist_directory=str(persist_path), embedding_function=embedding_fn)\n",
    "            end_time = time.time()\n",
    "            num_chunks = db._collection.count() if db and hasattr(db, '_collection') else 0\n",
    "            status_message += f\"ChromaDB loaded in {end_time - start_time:.2f}s from {persist_path}. Chunks: {num_chunks}.\"\n",
    "            if num_chunks == 0: status_message += \" Warning: Loaded DB is empty.\"\n",
    "            print(status_message)\n",
    "        except Exception as e:\n",
    "            err_msg = f\"Error loading ChromaDB from {persist_path}: {e}\"\n",
    "            print(err_msg)\n",
    "            return None, err_msg, 0\n",
    "    else:\n",
    "        return None, f\"Invalid mode: {mode}.\", 0\n",
    "    return db, status_message, num_chunks\n",
    "\n",
    "\n",
    "def list_sqlite_db_files(base_path: Path = BASE_DOCS_PATH) -> List[str]:\n",
    "    db_files = []\n",
    "    if not base_path.is_dir():\n",
    "        return [\"Error: Base document path not found or not a directory.\"]\n",
    "    \n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file_name in files:\n",
    "            if file_name.lower().endswith(('.db', '.sqlite', '.sqlite3')):\n",
    "                # Store relative path from base_path for display, or full path if preferred\n",
    "                relative_path = Path(root) / file_name\n",
    "                # Make it relative to base_path for cleaner display if it's a sub-path\n",
    "                try:\n",
    "                    display_path = str(relative_path.relative_to(base_path))\n",
    "                except ValueError: # If not a subpath (e.g., base_path itself is the file's dir)\n",
    "                    display_path = str(relative_path.name) # Or keep full path if desired\n",
    "                \n",
    "                # We need to be able to reconstruct the full path later\n",
    "                # So, maybe better to store (display_name, full_path_str) for Gradio Dropdown\n",
    "                db_files.append((str(relative_path), str(Path(root) / file_name))) # (Display, Value)\n",
    "                \n",
    "    if not db_files:\n",
    "        return [(\"No SQLite DBs found in 'docs' or its subdirectories.\", \"\")] # For (display, value)\n",
    "    \n",
    "    # Sort by display name\n",
    "    return sorted(db_files, key=lambda x: x[0])\n",
    "\n",
    "\n",
    "\n",
    "def list_potential_db_sources(base_path: Path = BASE_DOCS_PATH) -> List[str]:\n",
    "    sources = []\n",
    "    if not base_path.is_dir():\n",
    "        return [\"Error: Base document path not found or not a directory.\"]\n",
    "    for item_name in os.listdir(base_path):\n",
    "        item_path = base_path / item_name\n",
    "        if item_path.is_dir():\n",
    "            has_sqlite_for_creation = any(f.lower().endswith(('.db', '.sqlite', '.sqlite3')) for f in os.listdir(item_path))\n",
    "            is_chroma_dir_itself = (item_path / \"chroma.sqlite3\").exists()\n",
    "            has_chroma_subdir_with_file = (item_path / \"chroma_db\" / \"chroma.sqlite3\").exists()\n",
    "            if has_sqlite_for_creation or is_chroma_dir_itself or has_chroma_subdir_with_file:\n",
    "                sources.append(item_name)\n",
    "    if not sources:\n",
    "        return [\"No DB sources found. Check 'docs' subdirs for .db/.sqlite files or ChromaDB structures.\"]\n",
    "    return sorted(sources)\n",
    "\n",
    "def list_sqlite_files_in_folder(folder_path_str: Optional[str]) -> List[str]:\n",
    "    if not folder_path_str:\n",
    "        return []\n",
    "    folder_path = Path(folder_path_str)\n",
    "    if not folder_path.is_dir():\n",
    "        return []\n",
    "    \n",
    "    sqlite_files = [f.name for f in folder_path.iterdir() if f.is_file() and f.name.lower().endswith(('.db', '.sqlite', '.sqlite3'))]\n",
    "    return sorted(sqlite_files)\n",
    "\n",
    "\n",
    "def update_rag_sqlite_selector(selected_source_folder_name: str, db_mode: str):\n",
    "    if db_mode == \"Create New ChromaDB (from SQLite)\" and selected_source_folder_name and \\\n",
    "       not selected_source_folder_name.startswith(\"Error\") and \\\n",
    "       not selected_source_folder_name.startswith(\"No DB sources found\"):\n",
    "        \n",
    "        source_collection_path = BASE_DOCS_PATH / selected_source_folder_name\n",
    "        sqlite_files = list_sqlite_files_in_folder(str(source_collection_path))\n",
    "        \n",
    "        if sqlite_files:\n",
    "            return gr.update(choices=sqlite_files, value=sqlite_files[0] if sqlite_files else None, visible=True)\n",
    "        else:\n",
    "            return gr.update(choices=[], value=None, visible=True, # Show but indicate no files\n",
    "                             info=\"No SQLite files found in the selected folder.\") \n",
    "    else:\n",
    "        return gr.update(choices=[], value=None, visible=False) # Hide if not creating or no source\n",
    "    \n",
    "# --- Prompt Template & History Conversion ---\n",
    "template_str = \"\"\"\n",
    "Use the following Retrieved Documents and the previous conversation to answer the query. Incorporate your own knowledge and reasoning as an AI assistant. Keep the content together with document and chunk numbers at the end of sentences in brackets. If content from different documents is combined into a new sentence, place the document and chunk numbers at theend of the sentence, separated by semicolons.\n",
    "Previous Conversation: {history}\n",
    "Query: {query}\n",
    "Retrieved Documents: {retrieved_docs}\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"history\", \"query\", \"retrieved_docs\", \"answer\"], template=template_str\n",
    ")\n",
    "def convert_from_gradio_chat(gradio_chat_history: List[Tuple[Optional[str], Optional[str]]]) -> List[Dict[str, str]]:\n",
    "    app_history = []\n",
    "    for user_msg, ai_msg in gradio_chat_history:\n",
    "        if user_msg: app_history.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        if ai_msg: app_history.append({\"role\": \"assistant\", \"content\": ai_msg})\n",
    "    return app_history\n",
    "\n",
    "# --- Main Chat Interaction Function ---\n",
    "def handle_chat_interaction_gradio(query_text: str, chat_history_tuples: List[Tuple[Optional[str], Optional[str]]],\n",
    "                                   selected_method_value: str, k_value: int, vectordb_state: Optional[Chroma]):\n",
    "    start_time = time.time()\n",
    "    if not vectordb_state:\n",
    "        err_msg = \"VectorDB not loaded. Please load or create a DB first using the 'Database Management' section.\"\n",
    "        updated_history = chat_history_tuples + [[query_text, err_msg]]\n",
    "        yield (updated_history, query_text, \"Error: No DB\", \"Error: No DB\", \"Error: No DB\", err_msg)\n",
    "        return\n",
    "\n",
    "    app_conv_history = convert_from_gradio_chat(chat_history_tuples)\n",
    "    managed_history_str = manage_conversation_history(app_conv_history)\n",
    "    \n",
    "    doc_retriever = DocumentRetrieverClass(vectordb_state, openai_client=oai_client)\n",
    "    retrieved_docs_str, used_query_for_retrieval, _ = doc_retriever.retrieve_documents(\n",
    "        query_text, is_first_run=(not app_conv_history), k=k_value, method=selected_method_value\n",
    "    )\n",
    "\n",
    "    used_query_display = f\"**Used Retrieval Query:**  \\n{used_query_for_retrieval}\\n\"\n",
    "    query_to_format = query_text\n",
    "    if '{no history}' in query_text:\n",
    "        history_for_prompt = \"\"\n",
    "        query_to_format = query_text.replace('{no history}', '').strip()\n",
    "    else:\n",
    "        history_for_prompt = managed_history_str\n",
    "    \n",
    "    current_prompt = prompt_template.format(\n",
    "        history=history_for_prompt, query=query_to_format, retrieved_docs=retrieved_docs_str, answer=\"\"\n",
    "    )\n",
    "    prompt_display_text = current_prompt \n",
    "\n",
    "    retrieval_end_time = time.time()\n",
    "    retrieval_duration = retrieval_end_time - start_time\n",
    "    retrieved_tokens_count = word_count(retrieved_docs_str)\n",
    "    retrieval_time_msg = (f\"Retrieval: {retrieval_duration:.2f}s | Tokens: {retrieved_tokens_count} | Method: {selected_method_value} | k: {k_value}\")\n",
    "\n",
    "    yield (chat_history_tuples, query_text, prompt_display_text, used_query_display, retrieval_time_msg, \"Waiting for LLM...\")\n",
    "\n",
    "    messages_for_llm = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a scientific document analysis AI.\"},\n",
    "        {\"role\": \"user\", \"content\": current_prompt}\n",
    "    ]\n",
    "    \n",
    "    if not oai_client:\n",
    "        err_msg = \"OpenAI client not available. Cannot contact LLM.\"\n",
    "        updated_history = chat_history_tuples + [[query_text, err_msg]]\n",
    "        yield (updated_history, query_text, prompt_display_text, used_query_display, retrieval_time_msg, err_msg)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        completion = oai_client.chat.completions.create(\n",
    "            model=\"lmstudio/Meta-Llama-3.1\", messages=messages_for_llm, temperature=0.7, stream=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        err_msg = f\"LLM API Error: {e}\"\n",
    "        updated_history = chat_history_tuples + [[query_text, err_msg]]\n",
    "        yield (updated_history, query_text, prompt_display_text, used_query_display, retrieval_time_msg, err_msg)\n",
    "        return\n",
    "\n",
    "    full_response = \"\"\n",
    "    current_chat_history_for_display = chat_history_tuples + [[query_text, \"\"]]\n",
    "    llm_start_time = time.time()\n",
    "    for chunk in completion:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            full_response += chunk.choices[0].delta.content\n",
    "            current_chat_history_for_display[-1][1] = full_response\n",
    "            yield (current_chat_history_for_display, query_text, prompt_display_text, used_query_display, retrieval_time_msg, \"Streaming LLM response...\")\n",
    "    \n",
    "    llm_end_time = time.time()\n",
    "    message_tokens_llm = word_count(str(messages_for_llm))\n",
    "    history_tokens_llm = word_count(managed_history_str)\n",
    "    total_interaction_time = llm_end_time - start_time\n",
    "    gpt_response_time_msg = (f\"Total Interaction: {total_interaction_time:.2f}s (LLM: {llm_end_time - llm_start_time:.2f}s) | \"\n",
    "                             f\"LLM In Tokens (approx): {message_tokens_llm} | Hist Tokens (approx): {history_tokens_llm}\")\n",
    "    yield (current_chat_history_for_display, \"\", prompt_display_text, used_query_display, retrieval_time_msg, gpt_response_time_msg)\n",
    "\n",
    "\n",
    "# --- UI Definition ---\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"Scientific Document Assistant\") as demo:\n",
    "    vectordb_state = gr.State(None) # For RAG ChromaDB\n",
    "    sqlite_viewer_conn_state = gr.State(None) # For SQLite viewer connection (optional, can reconnect each time)\n",
    "    \n",
    "    # Load initial ingestion settings\n",
    "    initial_ingestion_settings = load_ingestion_settings(INGESTION_SETTINGS_FILE)\n",
    "    if not isinstance(initial_ingestion_settings, dict): initial_ingestion_settings = {}\n",
    "\n",
    "\n",
    "    with gr.Tab(\" Scientific RAG Conversation\"):\n",
    "        gr.Markdown(\"#  Scientific RAG Conversation\")\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=300):\n",
    "                gr.Markdown(\"###  RAG Database Management\")\n",
    "                db_source_dropdown = gr.Dropdown(\n",
    "                    label=\"Select RAG Document Collection Source Folder\", # Clarified label\n",
    "                    choices=list_potential_db_sources(),\n",
    "                    info=\"Select a subfolder from your 'docs' directory.\"\n",
    "                )\n",
    "                db_mode_radio = gr.Radio(\n",
    "                    choices=[\"Load Existing ChromaDB\", \"Create New ChromaDB (from SQLite)\"],\n",
    "                    value=\"Load Existing ChromaDB\", label=\"Action for RAG DB\"\n",
    "                )\n",
    "                # NEW Dropdown for specific SQLite file (initially hidden or empty)\n",
    "                rag_sqlite_file_dropdown = gr.Dropdown(\n",
    "                    label=\"Select Specific SQLite File for New RAG DB\",\n",
    "                    choices=[],\n",
    "                    interactive=True,\n",
    "                    visible=False, # Initially hidden\n",
    "                    info=\"Visible when 'Create New ChromaDB' is selected and source folder has SQLite files.\"\n",
    "                )\n",
    "                force_overwrite_checkbox = gr.Checkbox(\n",
    "                    label=\"Force Overwrite (if creating RAG DB and target ChromaDB dir exists)\", value=False\n",
    "                )\n",
    "                process_db_button = gr.Button(\" Process Selected RAG Database\", variant=\"primary\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                db_status_message = gr.Markdown(\"RAG DB Status: No Database Loaded.\")\n",
    "                num_docs_loaded_info = gr.Markdown(\"Original Docs in RAG Source: 0 | Chunks in RAG DB: 0\")\n",
    "\n",
    "                gr.Markdown(\"---\")\n",
    "                gr.Markdown(\"###  Chat Controls\")\n",
    "                selected_method_dd = gr.Dropdown(label='Retrieval Keyword Generation Method',\n",
    "                                                 choices=['combined', 'keywords', 'llm', 'original_query'], value='combined',\n",
    "                                                 info=\"How to refine query for retrieval. 'original_query' uses input as is.\")\n",
    "                k_value_slider = gr.Slider(minimum=1, maximum=50, value=10, step=1, label='Number of Chunks to Retrieve (K)')\n",
    "\n",
    "            with gr.Column(scale=3):\n",
    "                chatbot_display = gr.Chatbot(label=\"Conversation\", height=600, bubble_full_width=False, show_label=False)\n",
    "                query_input_box = gr.Textbox(label=\"Enter your query:\", placeholder=\"Type your message (e.g., 'doc_id:123' or natural language) and press Enter...\",\n",
    "                                             lines=3, show_label=False, elem_id=\"query-input-box\")\n",
    "\n",
    "        with gr.Accordion(\" Timings and Debug Information (RAG Chat)\", open=False):\n",
    "            retrieval_time_md = gr.Markdown(\"Retrieval Time: N/A\")\n",
    "            response_time_md = gr.Markdown(\"LLM Response Time: N/A\")\n",
    "            used_query_md = gr.Markdown(\"Used Retrieval Query: N/A\")\n",
    "            prompt_display_md = gr.Markdown(\"Full Prompt to LLM: N/A\")\n",
    "\n",
    "\n",
    "    with gr.Tab(\" Data Ingestion & DB Management\"):\n",
    "        gr.Markdown(\"#  Data Ingestion & SQLite Database Management\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2):\n",
    "                gr.Markdown(\"### 1. Process PDF/Text to SQLite Database\")\n",
    "\n",
    "                # Change Textbox to File component for uploading\n",
    "                ingest_input_files = gr.File(\n",
    "                    label=\"Upload PDF Files or a single TXT File\",\n",
    "                    file_count=\"multiple\",  # Allows multiple PDFs or one TXT\n",
    "                    file_types=[\".pdf\", \".txt\"],\n",
    "                    # value=initial_ingestion_settings.get('ingest_input_path', None) # .File doesn't easily take a default path string value\n",
    "                    # We'll handle \"default\" or remembered paths differently if needed, perhaps by showing the last processed path.\n",
    "                )\n",
    "                ingest_is_directory_mode = gr.Checkbox(\n",
    "                    label=\"Alternatively, process files from a server directory path (instead of uploads)\", \n",
    "                    value=False # Default to upload mode\n",
    "                )\n",
    "                ingest_server_directory_path = gr.Textbox(\n",
    "                    label=\"Server Directory Path (if checkbox above is ticked)\",\n",
    "                    placeholder=\"e.g., /path/to/your/pdfs_on_server\",\n",
    "                    value=initial_ingestion_settings.get('ingest_server_directory_path', ''), # New setting\n",
    "                    visible=False # Initially hidden\n",
    "                )\n",
    "\n",
    "                # Show/hide server directory path based on checkbox\n",
    "                def toggle_server_path_visibility(is_directory_mode_checked):\n",
    "                    return gr.update(visible=is_directory_mode_checked)\n",
    "\n",
    "                ingest_is_directory_mode.change(\n",
    "                    fn=toggle_server_path_visibility,\n",
    "                    inputs=[ingest_is_directory_mode],\n",
    "                    outputs=[ingest_server_directory_path]\n",
    "                )\n",
    "\n",
    "                ingest_output_dir = gr.Textbox(\n",
    "                    label=\"Output Directory for SQLite DB\",\n",
    "                    placeholder=\"e.g., /path/to/output_dbs (DB will be saved here)\",\n",
    "                    value=initial_ingestion_settings.get('ingest_output_dir', str(project_root_path / \"processed_databases\"))\n",
    "                )\n",
    "                ingest_db_name_stem = gr.Textbox(\n",
    "                    label=\"SQLite Database Name (without .db)\",\n",
    "                    placeholder=\"e.g., my_collection\",\n",
    "                    value=initial_ingestion_settings.get('ingest_db_name_stem', 'processed_docs')\n",
    "                )\n",
    "                ingest_processing_mode = gr.Radio(\n",
    "                    choices=[\"grobid\", \"text\", \"both\"], value=initial_ingestion_settings.get('ingest_processing_mode', \"grobid\"),\n",
    "                    label=\"Processing Mode\", info=\"Grobid for PDFs, Text for structured TXT, Both to try based on file type.\"\n",
    "                )\n",
    "                ingest_overwrite_db = gr.Checkbox(label=\"Overwrite SQLite DB if it exists\", value=initial_ingestion_settings.get('ingest_overwrite_db', False))\n",
    "                ingest_grobid_config = gr.Textbox(\n",
    "                    label=\"GROBID Config Path (optional)\", placeholder=\"config.json\",\n",
    "                    value=initial_ingestion_settings.get('ingest_grobid_config', 'config.json'),\n",
    "                    info=\"Path to GrobidClient's config.json, if not in CWD.\"\n",
    "                )\n",
    "\n",
    "                ingest_process_button = gr.Button(\" Process Files to SQLite\", variant=\"primary\")\n",
    "                ingest_status_md = gr.Markdown(\"Ingestion Status: Ready\")\n",
    "                ingest_output_db_path_md = gr.Markdown(\"Created DB Path: N/A\")\n",
    "\n",
    "        # ...the SQLite viewer column ...\n",
    "\n",
    "            with gr.Column(scale=3):\n",
    "                gr.Markdown(\"### 2. View SQLite Database Records\")\n",
    "                \n",
    "                # Option 1: Keep Textbox for manual path entry AND add a dropdown\n",
    "                view_sqlite_db_path_dropdown = gr.Dropdown(\n",
    "                    label=\"Select an existing SQLite DB to view (from 'docs' folder)\",\n",
    "                    choices=list_sqlite_db_files(), # This will be a list of (display, value)\n",
    "                    interactive=True,\n",
    "                    # info=\"Or paste path below and click 'Load SQLite DB'.\"\n",
    "                )\n",
    "                view_sqlite_db_path_textbox = gr.Textbox(\n",
    "                    label=\"Path to SQLite DB file (manual entry or auto-filled from selection/creation)\", \n",
    "                    placeholder=\"Select from dropdown, or paste path, or will use DB from Ingestion tab\"\n",
    "                )\n",
    "                view_load_db_button = gr.Button(\" Load SQLite DB for Viewing\")\n",
    "                \n",
    "                view_table_name_info = gr.Markdown(\"Table: N/A\")\n",
    "                \n",
    "                view_record_dropdown = gr.Dropdown(label=\"Select Record (ID: Title)\", choices=[], interactive=True)\n",
    "                view_record_details_md = gr.Markdown(\"Record Number: N/A | Author: N/A | Date: N/A\")\n",
    "                view_record_content_text = gr.Textbox(label=\"Record Content (Abstract / Body)\", lines=15, interactive=False, autoscroll=False)\n",
    "                view_status_md = gr.Markdown(\"Viewer Status: Ready\")\n",
    "\n",
    "    # --- RAG DB Processing Logic ---\n",
    "    def process_database_selection_ui(\n",
    "        selected_source_folder_name: str, # From db_source_dropdown\n",
    "        db_mode: str,                     # From db_mode_radio\n",
    "        selected_sqlite_file_name: Optional[str], # NEW: From rag_sqlite_file_dropdown\n",
    "        overwrite_flag: bool              # From force_overwrite_checkbox\n",
    "    ) -> Tuple[Optional[Chroma], str, str]:\n",
    "\n",
    "        if not selected_source_folder_name or \\\n",
    "           selected_source_folder_name.startswith(\"Error\") or \\\n",
    "           selected_source_folder_name.startswith(\"No DB sources found\"):\n",
    "            return None, \"Error: No valid RAG DB source folder selected.\", \"Original Docs: 0 | Chunks in DB: 0\"\n",
    "\n",
    "        source_collection_path = BASE_DOCS_PATH / selected_source_folder_name\n",
    "        new_vectordb = None\n",
    "        status_msg = \"\"\n",
    "        total_original_docs = 0\n",
    "        num_db_chunks = 0\n",
    "        # determined_chroma_persist_dir_str = \"\" # Will be set specifically\n",
    "\n",
    "        if db_mode == \"Load Existing ChromaDB\":\n",
    "            # Logic for loading existing ChromaDB:\n",
    "            # Try to find a ChromaDB named after a SQLite file stem, or default to 'chroma_db'\n",
    "            \n",
    "            # Option 1: Look for a ChromaDB that might be named after a SQLite file in the dir\n",
    "            # This is a bit heuristic. If there are multiple .db files and multiple chroma_xx_db dirs,\n",
    "            # we might need a more explicit way to link them or just pick one.\n",
    "            # For now, let's prioritize a default 'chroma_db' or one directly in the source_collection_path.\n",
    "\n",
    "            chroma_path_default_subdir = source_collection_path / \"chroma_db\" # Default location\n",
    "            chroma_path_in_source_dir = source_collection_path # Check if source_collection_path itself is a ChromaDB dir\n",
    "\n",
    "            determined_chroma_persist_dir_str = None\n",
    "\n",
    "            if (chroma_path_in_source_dir / \"chroma.sqlite3\").exists():\n",
    "                determined_chroma_persist_dir_str = str(chroma_path_in_source_dir)\n",
    "                status_msg += f\"Found ChromaDB directly in '{selected_source_folder_name}'. \"\n",
    "            elif (chroma_path_default_subdir / \"chroma.sqlite3\").exists():\n",
    "                determined_chroma_persist_dir_str = str(chroma_path_default_subdir)\n",
    "                status_msg += f\"Found ChromaDB in default '{selected_source_folder_name}/chroma_db'. \"\n",
    "            else:\n",
    "                # Attempt to find any subdirectory that looks like a Chroma DB\n",
    "                # This is more complex if you have multiple (e.g. chroma_ligase_db, chroma_kinase_db)\n",
    "                # For simplicity, if the above aren't found, we report an error.\n",
    "                # A more advanced version would list available ChromaDBs in the folder.\n",
    "                msg = f\"Error: No standard ChromaDB found in '{selected_source_folder_name}' (checked . and ./chroma_db/). \"\n",
    "                status_msg += msg\n",
    "                # To load a specific named ChromaDB (e.g. chroma_ligase_db), the user would need to select it.\n",
    "                # For now, this simple load looks for the default or root.\n",
    "                return None, status_msg, \"Original Docs: 0 | Chunks in DB: 0\"\n",
    "\n",
    "\n",
    "            new_vectordb, load_status_msg, num_db_chunks = create_or_load_chromadb(\n",
    "                None, embedding_function, determined_chroma_persist_dir_str, mode=\"load\"\n",
    "            )\n",
    "            status_msg += load_status_msg\n",
    "            \n",
    "            # Try to find an associated SQLite to get original doc count\n",
    "            # If multiple SQLite, pick the first one or try to match ChromaDB name if possible (more complex)\n",
    "            sqlite_files_in_folder = list_sqlite_files_in_folder(str(source_collection_path))\n",
    "            if new_vectordb and sqlite_files_in_folder:\n",
    "                # Heuristic: if ChromaDB is named like 'chroma_X_db', try to find 'X.db'\n",
    "                # This is a simplification.\n",
    "                sqlite_to_count_from = sqlite_files_in_folder[0] # Default to first\n",
    "                if determined_chroma_persist_dir_str:\n",
    "                    chroma_dir_name = Path(determined_chroma_persist_dir_str).name\n",
    "                    if chroma_dir_name.startswith(\"chroma_\") and chroma_dir_name.endswith(\"_db\"):\n",
    "                        potential_sqlite_stem = chroma_dir_name[len(\"chroma_\"):-len(\"_db\")]\n",
    "                        for s_file in sqlite_files_in_folder:\n",
    "                            if Path(s_file).stem == potential_sqlite_stem:\n",
    "                                sqlite_to_count_from = s_file\n",
    "                                break\n",
    "                try:\n",
    "                    _, _, total_original_docs = load_docs_from_sqlite(str(source_collection_path / sqlite_to_count_from))\n",
    "                except Exception:\n",
    "                    total_original_docs = -1 \n",
    "        \n",
    "        elif db_mode == \"Create New ChromaDB (from SQLite)\":\n",
    "            if not selected_sqlite_file_name:\n",
    "                msg = \"Error: No specific SQLite file selected for new RAG DB creation.\"\n",
    "                return None, msg, \"Original Docs: 0 | Chunks in DB: 0\"\n",
    "\n",
    "            sqlite_db_path = source_collection_path / selected_sqlite_file_name\n",
    "            if not sqlite_db_path.exists():\n",
    "                msg = f\"Error: Selected SQLite file '{selected_sqlite_file_name}' not found in '{selected_source_folder_name}'.\"\n",
    "                return None, msg, \"Original Docs: 0 | Chunks in DB: 0\"\n",
    "\n",
    "            # Customize ChromaDB directory name\n",
    "            sqlite_stem = Path(selected_sqlite_file_name).stem\n",
    "            # Sanitize stem for directory name (e.g., replace spaces, special chars if any)\n",
    "            safe_stem = \"\".join(c if c.isalnum() or c in ['_', '-'] else '_' for c in sqlite_stem)\n",
    "            chroma_db_dir_name = f\"chroma_{safe_stem}_db\"\n",
    "            determined_chroma_persist_dir = source_collection_path / chroma_db_dir_name\n",
    "\n",
    "            print(f\"Loading docs from SQLite: {sqlite_db_path} for new ChromaDB creation.\")\n",
    "            docs_from_sqlite, load_msg, total_original_docs = load_docs_from_sqlite(str(sqlite_db_path))\n",
    "            status_msg += load_msg + \" \"\n",
    "            \n",
    "            if not docs_from_sqlite:\n",
    "                return None, status_msg, f\"Original Docs: {total_original_docs} | Chunks in DB: 0\"\n",
    "\n",
    "            print(f\"Chunking {len(docs_from_sqlite)} documents...\")\n",
    "            chunked_texts = chunk_texts_with_metadata(docs_from_sqlite)\n",
    "            # unique_doc_ids = len(set(chunk.metadata.get('doc_id') for chunk in chunked_texts)) # Already printed in chunk_texts_with_metadata\n",
    "            status_msg += f\"Chunked into {len(chunked_texts)} pieces. \" # from {unique_doc_ids} documents. \"\n",
    "            \n",
    "            print(f\"Creating new ChromaDB in: {determined_chroma_persist_dir}\")\n",
    "            new_vectordb, create_load_msg, num_db_chunks = create_or_load_chromadb(\n",
    "                chunked_texts, embedding_function, str(determined_chroma_persist_dir), \n",
    "                mode=\"create\", force_overwrite=overwrite_flag\n",
    "            )\n",
    "            status_msg += create_load_msg\n",
    "        \n",
    "        num_docs_info_str = f\"Original Docs (SQLite source): {total_original_docs if total_original_docs != -1 else 'N/A'} | Chunks in RAG DB: {num_db_chunks}\"\n",
    "        if not new_vectordb and db_mode == \"Load Existing ChromaDB\" and num_db_chunks == 0 and \"Error\" not in status_msg:\n",
    "            status_msg += \" Loaded an empty Chroma DB.\" # Clarify if empty but no error\n",
    "        elif not new_vectordb:\n",
    "             num_docs_info_str = f\"Original Docs (SQLite source): {total_original_docs if total_original_docs != -1 else 'N/A'} | Chunks in RAG DB: 0 (Failed or not processed)\"\n",
    "        \n",
    "        return new_vectordb, status_msg, num_docs_info_str\n",
    "\n",
    "    process_db_button.click(\n",
    "        fn=process_database_selection_ui,\n",
    "        inputs=[\n",
    "            db_source_dropdown,\n",
    "            db_mode_radio,\n",
    "            rag_sqlite_file_dropdown, # NEW INPUT\n",
    "            force_overwrite_checkbox\n",
    "        ],\n",
    "        outputs=[vectordb_state, db_status_message, num_docs_loaded_info]\n",
    "    )\n",
    "\n",
    "\n",
    "    # --- Initial Greeting for Chatbot ---\n",
    "    def simple_initial_greeting_ui() -> List[Tuple[Optional[str], Optional[str]]]:\n",
    "        query_text = \"Hello! I am a scientific RAG assistant. Please load a RAG database from the 'RAG Database Management' panel to begin chatting, or go to the 'Data Ingestion' tab to process new documents.\"\n",
    "        return [[None, query_text]]\n",
    "    \n",
    "    # --- Data Ingestion Tab Callbacks ---\n",
    "    def handle_ingest_process_button_click(\n",
    "            uploaded_files_list: Optional[List[Any]], # List of tempfile._TemporaryFileWrapper from gr.File\n",
    "            is_directory_mode: bool,\n",
    "            server_dir_path: str,\n",
    "            output_dir: str, \n",
    "            db_name_stem: str, \n",
    "            mode: str, \n",
    "            overwrite: bool, \n",
    "            grobid_cfg: str, \n",
    "            progress=gr.Progress(track_tqdm=True)):\n",
    "        \n",
    "        # Save settings - decide what to save for the input path now\n",
    "        current_ingestion_settings = {\n",
    "            # 'ingest_input_path': ..., # This is tricky with gr.File, maybe save last server_dir_path\n",
    "            'ingest_server_directory_path': server_dir_path if is_directory_mode else initial_ingestion_settings.get('ingest_server_directory_path', ''), # Save last used server path\n",
    "            'ingest_output_dir': output_dir,\n",
    "            'ingest_db_name_stem': db_name_stem,\n",
    "            'ingest_processing_mode': mode,\n",
    "            'ingest_overwrite_db': overwrite,\n",
    "            'ingest_grobid_config': grobid_cfg\n",
    "        }\n",
    "        save_ingestion_settings(current_ingestion_settings, INGESTION_SETTINGS_FILE)\n",
    "\n",
    "        input_target_path_for_processor: Optional[str] = None\n",
    "        temp_upload_dir: Optional[Path] = None # To store uploaded files temporarily if needed\n",
    "\n",
    "        if is_directory_mode:\n",
    "            if not server_dir_path:\n",
    "                return \"Error: Server Directory Path is required when 'process from server directory' is checked.\", \"N/A\", gr.update()\n",
    "            input_target_path_for_processor = server_dir_path\n",
    "        else: # Upload mode\n",
    "            if not uploaded_files_list:\n",
    "                return \"Error: No files uploaded. Please upload PDF(s) or a TXT file.\", \"N/A\", gr.update()\n",
    "            \n",
    "            # Create a temporary directory to store uploaded files before processing\n",
    "            # This makes it easier for `process_documents_to_sqlite` if it expects a directory.\n",
    "            temp_upload_dir = project_root_path / \"temp_uploads\" / str(time.time_ns())\n",
    "            temp_upload_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            processed_one_file = False\n",
    "            for uploaded_file_obj in uploaded_files_list:\n",
    "                original_filename = Path(uploaded_file_obj.name).name # Get original filename\n",
    "                temp_save_path = temp_upload_dir / original_filename\n",
    "                try:\n",
    "                    # Gradio's temp file needs to be copied to a persistent temp location\n",
    "                    # because the original temp file wrapper might be closed or deleted.\n",
    "                    shutil.copy(uploaded_file_obj.name, temp_save_path)\n",
    "                    print(f\"Copied uploaded file {original_filename} to {temp_save_path}\")\n",
    "                    processed_one_file = True\n",
    "                except Exception as e:\n",
    "                    shutil.rmtree(temp_upload_dir, ignore_errors=True)\n",
    "                    return f\"Error copying uploaded file {original_filename}: {e}\", \"N/A\", gr.update()\n",
    "\n",
    "            if not processed_one_file:\n",
    "                shutil.rmtree(temp_upload_dir, ignore_errors=True)\n",
    "                return \"Error: Could not process any of the uploaded files.\", \"N/A\", gr.update()\n",
    "\n",
    "            input_target_path_for_processor = str(temp_upload_dir)\n",
    "            # Note: If `process_documents_to_sqlite` can handle a list of file paths directly,\n",
    "            # you might not need to copy them all to one temp_upload_dir.\n",
    "            # But sending a directory is often simpler for existing directory-based processors.\n",
    "\n",
    "        if not output_dir or not db_name_stem:\n",
    "            if temp_upload_dir: shutil.rmtree(temp_upload_dir, ignore_errors=True)\n",
    "            return \"Error: Output Directory and DB Name Stem are required.\", \"N/A\", gr.update()\n",
    "\n",
    "        status_msg, created_db_path = process_documents_to_sqlite(\n",
    "            input_target_path_for_processor, # This is now either server_dir_path or path to temp_upload_dir\n",
    "            output_dir, db_name_stem, mode, overwrite, grobid_cfg,\n",
    "            progress_callback=lambda p, desc: progress(p, desc=desc)\n",
    "        )\n",
    "        \n",
    "        # Clean up temporary upload directory if it was used\n",
    "        if temp_upload_dir:\n",
    "            shutil.rmtree(temp_upload_dir, ignore_errors=True)\n",
    "            print(f\"Cleaned up temporary upload directory: {temp_upload_dir}\")\n",
    "\n",
    "        if created_db_path:\n",
    "            # Clear the file input component after successful processing (optional)\n",
    "            return status_msg, f\"Created DB: {created_db_path}\", created_db_path, gr.update(value=None) # Clears gr.File\n",
    "        else:\n",
    "            return status_msg, \"Failed to create DB or no DB path returned.\", gr.update(), gr.update(value=None) # Clears gr.File\n",
    "\n",
    "    # Update the ingest_process_button.click call\n",
    "    ingest_process_button.click(\n",
    "        fn=handle_ingest_process_button_click,\n",
    "        inputs=[\n",
    "            ingest_input_files, ingest_is_directory_mode, ingest_server_directory_path,\n",
    "            ingest_output_dir, ingest_db_name_stem, ingest_processing_mode, \n",
    "            ingest_overwrite_db, ingest_grobid_config\n",
    "        ],\n",
    "        outputs=[ingest_status_md, ingest_output_db_path_md, view_sqlite_db_path_textbox, ingest_input_files] # Add ingest_input_files to clear it\n",
    "    )\n",
    "\n",
    "    # --- SQLite Viewer Callbacks ---\n",
    "    sqlite_viewer_db_connection = None # Module-level variable to hold connection if we want to keep it open\n",
    "\n",
    "    def load_sqlite_for_viewing(db_path_str: str):\n",
    "        # global sqlite_viewer_db_connection # Decide if you really need this global\n",
    "        # if sqlite_viewer_db_connection:\n",
    "        #     try: sqlite_viewer_db_connection.close()\n",
    "        #     except: pass\n",
    "        #     sqlite_viewer_db_connection = None\n",
    "\n",
    "        if not db_path_str or not Path(db_path_str).exists():\n",
    "            return \"Error: SQLite DB path is invalid or file does not exist.\", \"Table: N/A\", gr.update(choices=[], value=None), \"\", \"\", \"\"\n",
    "        \n",
    "        conn = None # Initialize conn\n",
    "        try:\n",
    "            conn = sqlite3.connect(f\"file:{db_path_str}?mode=ro\", uri=True)\n",
    "            # sqlite_viewer_db_connection = conn # If you keep it global\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='document_table';\")\n",
    "            table_info = cursor.fetchone()\n",
    "            if not table_info:\n",
    "                cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "                table_info = cursor.fetchone()\n",
    "\n",
    "            if not table_info:\n",
    "                conn.close()\n",
    "                return \"Error: No tables found in the database.\", \"Table: N/A\", gr.update(choices=[], value=None), \"\", \"\", \"\"\n",
    "            \n",
    "            table_name = table_info[0]\n",
    "\n",
    "            # Get total record count for user information\n",
    "            cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "            total_db_records = cursor.fetchone()[0]\n",
    "            \n",
    "            cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "            columns = [row[1] for row in cursor.fetchall()]\n",
    "            id_col_name = \"ID\" # Assuming 'ID' is your primary key column for selection\n",
    "            title_col_name = \"Title\" if \"Title\" in columns else (\"Abstract\" if \"Abstract\" in columns else None)\n",
    "            \n",
    "            record_options_for_gradio = [] # List of (display_str, actual_id_value)\n",
    "\n",
    "            if not title_col_name: \n",
    "                cursor.execute(f\"SELECT {id_col_name} FROM {table_name} ORDER BY {id_col_name}\")\n",
    "                fetched_for_dropdown = cursor.fetchall()\n",
    "                for row_tuple in fetched_for_dropdown:\n",
    "                    record_id = row_tuple[0]\n",
    "                    display_str = f\"ID {record_id}\"\n",
    "                    record_options_for_gradio.append((display_str, record_id))\n",
    "            else:\n",
    "                cursor.execute(f\"SELECT {id_col_name}, {title_col_name} FROM {table_name} ORDER BY {id_col_name}\")\n",
    "                fetched_for_dropdown = cursor.fetchall()\n",
    "                for r_id, r_title in fetched_for_dropdown:\n",
    "                    title_str = str(r_title) if r_title else \"No Title\"\n",
    "                    display_title_text = title_str[:70] + \"...\" if len(title_str) > 70 else title_str\n",
    "                    display_str = f\"ID {r_id}: {display_title_text}\"\n",
    "                    record_options_for_gradio.append((display_str, r_id))\n",
    "            \n",
    "            first_val_actual_id = record_options_for_gradio[0][1] if record_options_for_gradio else None\n",
    "            status_text = f\"Loaded DB: {Path(db_path_str).name}. Table: {table_name} ({total_db_records} records).\"\n",
    "            \n",
    "            # Output for view_record_dropdown should be gr.update(choices=..., value=...)\n",
    "            # Output for view_table_name_info: string\n",
    "            # Output for view_record_details_md: string (empty initially)\n",
    "            # Output for view_record_content_text: string (empty initially)\n",
    "            # Output for view_status_md: string\n",
    "            return (status_text, \n",
    "                    f\"Table: {table_name} ({total_db_records} records)\", \n",
    "                    gr.update(choices=record_options_for_gradio, value=first_val_actual_id), \n",
    "                    \"\", # for view_record_details_md\n",
    "                    \"\", # for view_record_content_text\n",
    "                    \"DB Loaded. Select a record.\" # for view_status_md (final one)\n",
    "                   )\n",
    "\n",
    "        except Exception as e:\n",
    "            # import traceback # For debugging\n",
    "            # print(traceback.format_exc()) # For debugging\n",
    "            error_msg = f\"Error loading SQLite DB: {e}\"\n",
    "            return error_msg, \"Table: N/A\", gr.update(choices=[], value=None), \"\", \"\", error_msg\n",
    "        finally:\n",
    "            if conn:\n",
    "                conn.close()\n",
    "                # sqlite_viewer_db_connection = None # if global\n",
    "\n",
    "    def display_selected_sqlite_record(selected_record_actual_id: Optional[int], db_path_str: str):\n",
    "        if selected_record_actual_id is None or not db_path_str: # Check for None explicitly\n",
    "            return \"No record selected or DB path missing.\", \"\", \"\"\n",
    "\n",
    "        record_id = selected_record_actual_id # It's already an int. This is the ID to use.\n",
    "        conn = None\n",
    "\n",
    "        try:\n",
    "            # ------------------------------------------------------------- #\n",
    "            # DELETE THIS ENTIRE BLOCK:\n",
    "            # record_id_match = re.match(r\"ID (\\d+):?\", selected_record_str)\n",
    "            # if not record_id_match:\n",
    "            #     record_id_match = re.match(r\"ID (\\d+)\", selected_record_str)\n",
    "            #     if not record_id_match:\n",
    "            #         return \"Could not parse Record ID from selection.\", \"\", \"\"\n",
    "            # record_id = int(record_id_match.group(1))\n",
    "            # ------------------------------------------------------------- #\n",
    "\n",
    "            conn = sqlite3.connect(f\"file:{db_path_str}?mode=ro\", uri=True)\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Determine table name again\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='document_table';\")\n",
    "            table_info = cursor.fetchone()\n",
    "            if not table_info:\n",
    "                cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "                table_info = cursor.fetchone()\n",
    "            if not table_info: \n",
    "                if conn: conn.close() # Ensure connection is closed before returning\n",
    "                return \"Error: Table not found.\", \"\", \"\"\n",
    "            table_name = table_info[0]\n",
    "\n",
    "            # Fetch all relevant columns for the selected ID\n",
    "            cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "            columns = [row[1] for row in cursor.fetchall()]\n",
    "            \n",
    "            cols_to_fetch = [\"Abstract\", \"Body\", \"Authors\", \"Date\", \"Record_Number\", \"Journal\", \"Title\", \"DOI\", \"Refs\", \"Citations\"]\n",
    "            select_cols_str = \", \".join([col for col in cols_to_fetch if col in columns])\n",
    "            \n",
    "            if not select_cols_str: \n",
    "                select_cols_str = \"*\" \n",
    "\n",
    "            # Use the record_id (which is selected_record_actual_id) directly in the query\n",
    "            query = f\"SELECT {select_cols_str} FROM {table_name} WHERE ID = ?\" # Assuming your ID column is named 'ID'\n",
    "            cursor.execute(query, (record_id,)) # Pass record_id as a tuple\n",
    "            record_data_tuple = cursor.fetchone()\n",
    "            # conn.close() # Moved to finally block\n",
    "\n",
    "            if not record_data_tuple:\n",
    "                return f\"Record ID {record_id} not found.\", \"\", \"\"\n",
    "            \n",
    "            fetched_cols_list = [col for col in cols_to_fetch if col in columns and select_cols_str != \"*\"]\n",
    "            if select_cols_str == \"*\": # If we fetched all columns with *, get column names from cursor.description\n",
    "                fetched_cols_list = [desc[0] for desc in cursor.description]\n",
    "\n",
    "            record_data = dict(zip(fetched_cols_list, record_data_tuple))\n",
    "\n",
    "            title_text = str(record_data.get('Title', 'N/A'))\n",
    "            abstract_text = str(record_data.get('Abstract', ''))\n",
    "            body_text = str(record_data.get('Body', ''))\n",
    "            \n",
    "            display_content = f\"Title: {title_text}\\n\\n\"\n",
    "            if abstract_text and body_text and abstract_text.strip() == body_text.strip():\n",
    "                 display_content += f\"Abstract/Body:\\n{abstract_text}\"\n",
    "            else:\n",
    "                if abstract_text: display_content += f\"Abstract:\\n{abstract_text}\\n\\n\"\n",
    "                if body_text: display_content += f\"Body:\\n{body_text}\"\n",
    "            \n",
    "            if not abstract_text and not body_text and not title_text == 'N/A': # Show raw if no main content but title exists\n",
    "                 display_content += \"\\nNo Abstract or Body found. Raw data:\\n\" + \"\\n\".join([f\"{k}: {v}\" for k,v in record_data.items()])\n",
    "            elif not abstract_text and not body_text and title_text == 'N/A': # Truly empty\n",
    "                 display_content = \"No content found for this record.\"\n",
    "\n",
    "\n",
    "            details_str = (\n",
    "                f\"Record Number: {record_data.get('Record_Number', 'N/A')} | \"\n",
    "                f\"Authors: {str(record_data.get('Authors', 'N/A'))[:100]}... | \"\n",
    "                f\"Date: {record_data.get('Date', 'N/A')} | \"\n",
    "                f\"Journal: {record_data.get('Journal', 'N/A')} | \"\n",
    "                f\"DOI: {record_data.get('DOI', 'N/A')}\"\n",
    "            )\n",
    "            return details_str, display_content, \"Record displayed.\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            # import traceback # For debugging\n",
    "            # print(traceback.format_exc()) # For debugging\n",
    "            err_msg = f\"Error displaying record ID {record_id if 'record_id' in locals() else 'unknown'}: {e}\"\n",
    "            return err_msg, \"\", \"Error\"\n",
    "        \n",
    "        finally:\n",
    "            if conn:\n",
    "                conn.close()\n",
    "                \n",
    "                \n",
    "    # New callback to update textbox when dropdown selection changes\n",
    "    def update_viewer_path_from_dropdown(selected_db_full_path: str):\n",
    "        # selected_db_full_path is the 'value' from the (display, value) tuple\n",
    "        if selected_db_full_path:\n",
    "            return gr.update(value=selected_db_full_path)\n",
    "        return gr.update() # No change if nothing selected or empty value\n",
    "\n",
    "    view_sqlite_db_path_dropdown.change(\n",
    "        fn=update_viewer_path_from_dropdown,\n",
    "        inputs=[view_sqlite_db_path_dropdown],\n",
    "        outputs=[view_sqlite_db_path_textbox]\n",
    "    )\n",
    "    \n",
    "    view_load_db_button.click(\n",
    "        fn=load_sqlite_for_viewing, # Your existing function\n",
    "        inputs=[view_sqlite_db_path_textbox], # Use the textbox as the source of truth for the path\n",
    "        outputs=[view_status_md, view_table_name_info, view_record_dropdown, view_record_details_md, view_record_content_text, view_status_md]\n",
    "    )\n",
    "    \n",
    "    view_record_dropdown.change(\n",
    "        fn=display_selected_sqlite_record,\n",
    "        inputs=[view_record_dropdown, view_sqlite_db_path_textbox], # Pass db_path again\n",
    "        outputs=[view_record_details_md, view_record_content_text, view_status_md]\n",
    "    )\n",
    "    \n",
    "    db_source_dropdown.change(\n",
    "        fn=update_rag_sqlite_selector,\n",
    "        inputs=[db_source_dropdown, db_mode_radio],\n",
    "        outputs=[rag_sqlite_file_dropdown]\n",
    "    )\n",
    "    \n",
    "    db_mode_radio.change(\n",
    "        fn=update_rag_sqlite_selector,\n",
    "        inputs=[db_source_dropdown, db_mode_radio],\n",
    "        outputs=[rag_sqlite_file_dropdown]\n",
    "    )\n",
    "\n",
    "    # --- RAG Chat Input Submission ---\n",
    "    demo.load(fn=simple_initial_greeting_ui, inputs=None, outputs=[chatbot_display]) # Ensure inputs=None if no inputs\n",
    "    \n",
    "    query_input_box.submit(\n",
    "        fn=handle_chat_interaction_gradio,\n",
    "        inputs=[query_input_box, chatbot_display, selected_method_dd, k_value_slider, vectordb_state],\n",
    "        outputs=[chatbot_display, query_input_box, prompt_display_md, used_query_md, retrieval_time_md, response_time_md],\n",
    "        show_progress=\"full\"\n",
    "    )\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Script CWD: {os.getcwd()}\")\n",
    "    print(f\"Looking for RAG document collections in: {BASE_DOCS_PATH}\")\n",
    "    if not BASE_DOCS_PATH.exists():\n",
    "        print(f\"WARNING: Base RAG documents path {BASE_DOCS_PATH} does not exist! This is for the RAG tab.\")\n",
    "    \n",
    "    processed_db_default_dir = project_root_path / \"processed_databases\"\n",
    "    if not processed_db_default_dir.exists():\n",
    "        try:\n",
    "            processed_db_default_dir.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"Created default directory for processed SQLite databases: {processed_db_default_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not create default directory for processed databases {processed_db_default_dir}: {e}\")\n",
    "\n",
    "    if not oai_client:\n",
    "        print(\"CRITICAL WARNING: OpenAI client (oai_client) is NOT initialized. LLM and Embedding features will FAIL.\")\n",
    "    \n",
    "    print(\"Launching Gradio App...\")\n",
    "    demo.queue().launch(debug=True, server_name=\"127.0.0.1\", share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617628a0-5417-4b55-9759-dae14d772e69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
