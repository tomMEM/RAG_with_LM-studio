{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a84502f6-d41e-447c-b9e2-f2cd4d9958f1",
   "metadata": {},
   "source": [
    " * config.json  with \n",
    "```text\n",
    "{\n",
    "  \"GEMINI_API_KEY\": \"Your Key\",\n",
    "  \"KIMI_API_KEY\":  \"Your Key\",\n",
    "  \"HTTP_PROXY_URL\": \"http://localhost:port\"\n",
    "} \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996711d9-1481-4437-a2b2-31fbd7ec390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funct\n",
    "import os\n",
    "import gradio as gr\n",
    "import traceback\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Function to check if a proxy is available\n",
    "import socket\n",
    "\n",
    "# Define the is_proxy_available function at the top of your script\n",
    "def is_proxy_available(url):\n",
    "    \"\"\"Checks if a proxy is available by trying to connect to its host and port.\"\"\"\n",
    "    try:\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.settimeout(1)  # Timeout after 1 second\n",
    "\n",
    "        # --- FIX IS HERE ---\n",
    "        # The host and port must be passed as a single tuple: (host, port)\n",
    "        address = (url[\"host\"], url[\"port\"])\n",
    "        result = sock.connect_ex(address)\n",
    "        # --- END OF FIX ---\n",
    "\n",
    "        if result == 0:\n",
    "            # A result of 0 means the connection was successful\n",
    "            return True\n",
    "    except socket.error as err:\n",
    "        # This can happen for various reasons (e.g., DNS resolution failure)\n",
    "        print(f\"Socket error while checking proxy: {err}\")\n",
    "    except KeyError:\n",
    "        # Handle cases where the url dict might be missing 'host' or 'port'\n",
    "        print(f\"Invalid URL dictionary passed to is_proxy_available: {url}\")\n",
    "    finally:\n",
    "        # Always close the socket to free up resources\n",
    "        sock.close()\n",
    "        \n",
    "    return False\n",
    "# --- Global Configuration ---\n",
    "CONFIG = {}\n",
    "\n",
    "# --- Global Variables ---\n",
    "kimi_client = None\n",
    "gemini_model = None\n",
    "current_model_technical_name = None\n",
    "gemini_configured = False\n",
    "kimi_configured = False\n",
    "\n",
    "AVAILABLE_MODELS = {\n",
    "    # --- Kimi (Moonshot AI) Models ---\n",
    "    # Using the stable, documented v1 models.\n",
    "    \"Kimi (32k context)\": \"moonshot-v1-32k\",\n",
    "    \"Kimi (128k context)\": \"moonshot-v1-128k\",\n",
    "\n",
    "    # --- Google Gemini Models ---\n",
    "    # The 'latest' tag ensures you get the most recent version automatically.\n",
    "    \"Gemini 1.5 Flash\": \"gemini-1.5-flash-latest\",\n",
    "    \"Gemini 2.5 Flash\": \"gemini-2.5-flash-latest\",\n",
    "    \"Gemini 1.5 Pro\": \"gemini-1.5-pro-latest\",\n",
    "    \"Gemini 2.5 Pro\": \"gemini-2.5-pro-latest\",\n",
    "    \n",
    "}\n",
    "\n",
    "DEFAULT_MODEL_DISPLAY_NAME = \"Kimi (32k context)\"\n",
    "\n",
    "def load_and_configure():\n",
    "    \"\"\"\n",
    "    Loads configuration from config.json and sets up the proxy globally.\n",
    "    This is the one-stop function for all initial setup.\n",
    "    \"\"\"\n",
    "    global CONFIG\n",
    "    try:\n",
    "        # Load from config.json in the current directory (for Jupyter)\n",
    "        config_path = os.path.join(os.getcwd(), 'config.json')\n",
    "        print(f\"Attempting to load configuration from: {config_path}\")\n",
    "        if os.path.exists(config_path):\n",
    "            with open(config_path, 'r') as f:\n",
    "                CONFIG = json.load(f)\n",
    "            print(\"Successfully loaded settings from config.json\")\n",
    "        else:\n",
    "            print(\"WARNING: config.json not found.\")\n",
    "            CONFIG = {}\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not read or parse config.json: {e}\")\n",
    "        CONFIG = {}\n",
    "\n",
    "    # Allow environment variables to override the config file\n",
    "    CONFIG['GEMINI_API_KEY'] = os.environ.get(\"GEMINI_API_KEY\", CONFIG.get('GEMINI_API_KEY'))\n",
    "    CONFIG['KIMI_API_KEY'] = os.environ.get(\"KIMI_API_KEY\", CONFIG.get('KIMI_API_KEY'))\n",
    "    CONFIG['HTTP_PROXY_URL'] = os.environ.get(\"HTTP_PROXY_URL\", CONFIG.get('HTTP_PROXY_URL'))\n",
    "\n",
    "    # --- SET PROXY GLOBALLY FOR ALL LIBRARIES ---\n",
    "    proxy_url = CONFIG.get(\"HTTP_PROXY_URL\")\n",
    "    if proxy_url:\n",
    "        # Parse the proxy URL to get the host and port\n",
    "        proxy_parts = proxy_url.split('://')[-1].split(':')\n",
    "        host = proxy_parts[0]\n",
    "        port = int(proxy_parts[-1])\n",
    "        if is_proxy_available({\"host\": host, \"port\": port}):\n",
    "            os.environ['http_proxy'] = proxy_url\n",
    "            os.environ['https_proxy'] = proxy_url\n",
    "            print(f\"Proxy set globally for this session: {proxy_url}\")\n",
    "        else:\n",
    "            print(\"Proxy is not available or not configured. Using direct connection.\")\n",
    "            # Remove proxy environment variables if they exist\n",
    "            os.environ.pop('http_proxy', None)\n",
    "            os.environ.pop('https_proxy', None)\n",
    "    else:\n",
    "        print(\"No proxy configured. Using direct connection.\")\n",
    "        # Remove proxy environment variables if they exist\n",
    "        os.environ.pop('http_proxy', None)\n",
    "        os.environ.pop('https_proxy', None)\n",
    "\n",
    "    # --- CRITICAL: Set NO_PROXY for Gradio's internal communication ---\n",
    "    current_no_proxy = os.environ.get('NO_PROXY', '')\n",
    "    additional_no_proxy_hosts = ['localhost', '127.0.0.1', '0.0.0.0']\n",
    "    new_no_proxy_parts = [host for host in current_no_proxy.split(',') if host.strip()]\n",
    "    for host in additional_no_proxy_hosts:\n",
    "        if host not in new_no_proxy_parts:\n",
    "            new_no_proxy_parts.append(host)\n",
    "    os.environ['NO_PROXY'] = ','.join(new_no_proxy_parts)\n",
    "    print(f\"NO_PROXY set for Gradio: {os.environ.get('NO_PROXY')}\")\n",
    "\n",
    "\n",
    "def initialize_api_clients():\n",
    "    \"\"\"Initializes the API clients using the globally set proxy.\"\"\"\n",
    "    global kimi_client, gemini_configured, kimi_configured\n",
    "    \n",
    "    # Initialize Gemini\n",
    "    gemini_api_key = CONFIG.get('GEMINI_API_KEY')\n",
    "    if gemini_api_key:\n",
    "        try:\n",
    "            # Let Gemini auto-detect the proxy from the environment\n",
    "            genai.configure(api_key=gemini_api_key)\n",
    "            gemini_configured = True\n",
    "            print(\"Google Gemini API configured successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to configure Gemini API: {e}\")\n",
    "    else:\n",
    "        print(\"INFO: GEMINI_API_KEY not found.\")\n",
    "\n",
    "    # Initialize Kimi\n",
    "    kimi_api_key = CONFIG.get('KIMI_API_KEY')\n",
    "    if kimi_api_key:\n",
    "        try:\n",
    "            # Let Kimi auto-detect the proxy from the environment\n",
    "            kimi_client = OpenAI(\n",
    "                api_key=kimi_api_key,\n",
    "                base_url=\"https://api.moonshot.cn/v1\",\n",
    "            )\n",
    "            kimi_configured = True\n",
    "            print(\"Kimi (Moonshot) client initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to initialize Kimi client: {e}\")\n",
    "    else:\n",
    "        print(\"INFO: KIMI_API_KEY not found.\")\n",
    "\n",
    "\n",
    "# --- Chat Responder Functions (Unchanged) ---\n",
    "def gemini_chat_responder(user_message, chat_history):\n",
    "    global gemini_model, current_model_technical_name\n",
    "    if gemini_model is None or gemini_model.model_name != f\"models/{current_model_technical_name}\":\n",
    "        gemini_model = genai.GenerativeModel(current_model_technical_name)\n",
    "    gemini_history = [{'role': 'user' if item[0] else 'model', 'parts': [item[0] if item[0] else item[1]]} for item in chat_history]\n",
    "    try:\n",
    "        chat_session = gemini_model.start_chat(history=gemini_history)\n",
    "        response = chat_session.send_message(user_message)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        traceback.print_exc(); return f\"An error occurred with Google Gemini: {e}\"\n",
    "\n",
    "def kimi_chat_responder(user_message, chat_history):\n",
    "    messages = [{\"role\": \"system\", \"content\": \"你是 Kimi，由 Moonshot AI 出品的超长上下文人工智能助手。\"}]\n",
    "    for user_msg, ai_msg in chat_history:\n",
    "        messages.extend([{'role': 'user', 'content': user_msg}, {'role': 'assistant', 'content': ai_msg}])\n",
    "    messages.append({'role': 'user', 'content': user_message})\n",
    "    try:\n",
    "        response = kimi_client.chat.completions.create(model=current_model_technical_name, messages=messages, temperature=0.7)\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        traceback.print_exc(); return f\"An error occurred with the Kimi API: {e}\"\n",
    "\n",
    "def master_chat_responder(user_message, chat_history):\n",
    "    global current_model_technical_name\n",
    "    if not user_message.strip(): return \"Please type a message.\"\n",
    "    if 'moonshot' in current_model_technical_name:\n",
    "        if not kimi_configured: return \"Error: Kimi API key not configured.\"\n",
    "        return kimi_chat_responder(user_message, chat_history)\n",
    "    elif 'gemini' in current_model_technical_name:\n",
    "        if not gemini_configured: return \"Error: Gemini API key not configured.\"\n",
    "        return gemini_chat_responder(user_message, chat_history)\n",
    "    else:\n",
    "        return \"Error: Unknown model type.\"\n",
    "\n",
    "# --- Gradio UI (Unchanged) ---\n",
    "with gr.Blocks(theme=\"soft\", title=\"Multi-LLM Chat\") as demo:\n",
    "    gr.Markdown(\"# Multi-LLM Chat Dashboard (Kimi & Gemini)\")\n",
    "    \n",
    "    # --- Run ALL setup first ---\n",
    "    load_and_configure()\n",
    "    initialize_api_clients()\n",
    "    \n",
    "    current_model_technical_name = AVAILABLE_MODELS.get(DEFAULT_MODEL_DISPLAY_NAME)\n",
    "    with gr.Row():\n",
    "        model_dropdown = gr.Dropdown(choices=list(AVAILABLE_MODELS.keys()), value=DEFAULT_MODEL_DISPLAY_NAME, label=\"Select LLM Model\")\n",
    "        model_status_text = gr.Textbox(label=\"Model Status\", value=f\"Current: {current_model_technical_name}\", interactive=False)\n",
    "    chat_interface = gr.ChatInterface(fn=master_chat_responder, chatbot=gr.Chatbot(height=500), retry_btn=\"Retry\", undo_btn=\"Undo\", clear_btn=\"Clear\")\n",
    "    def handle_model_change(selected_display_name):\n",
    "        global current_model_technical_name, gemini_model\n",
    "        technical_name = AVAILABLE_MODELS.get(selected_display_name)\n",
    "        if 'moonshot' in technical_name and not kimi_configured: return \"Cannot switch: Kimi API not configured.\"\n",
    "        if 'gemini' in technical_name and not gemini_configured: return \"Cannot switch: Gemini API not configured.\"\n",
    "        if current_model_technical_name and 'gemini' in current_model_technical_name: gemini_model = None\n",
    "        current_model_technical_name = technical_name\n",
    "        return f\"Switched to model: {current_model_technical_name}\"\n",
    "    model_dropdown.change(fn=handle_model_change, inputs=[model_dropdown], outputs=[model_status_text])\n",
    "    if not gemini_configured: gr.Markdown(\"<p style='color:orange;'>Warning: Gemini API not configured.</p>\")\n",
    "    if not kimi_configured: gr.Markdown(\"<p style='color:orange;'>Warning: Kimi API not configured.</p>\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(server_name=\"0.0.0.0\", share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allensdk",
   "language": "python",
   "name": "allensdk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
